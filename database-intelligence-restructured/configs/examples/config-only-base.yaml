# Base Configuration for Config-Only Mode
# This configuration uses only standard OpenTelemetry components
# No custom code or receivers required

# Environment variables required:
# - DB_ENDPOINT: Database connection string
# - DB_USERNAME: Database read-only username  
# - DB_PASSWORD: Database password
# - NEW_RELIC_OTLP_ENDPOINT: New Relic OTLP endpoint
# - NEW_RELIC_LICENSE_KEY: New Relic license key
# - SERVICE_NAME: Service identifier
# - ENVIRONMENT: Deployment environment (dev/staging/prod)

receivers:
  # PostgreSQL metrics receiver
  postgresql:
    endpoint: "${DB_ENDPOINT}"
    username: "${DB_USERNAME}"
    password: "${DB_PASSWORD}"
    databases:
      - "*"  # Monitor all databases
    collection_interval: 30s
    transport: tcp
    tls:
      insecure_skip_verify: false
    # Enable all available metrics
    metrics:
      postgresql.bgwriter.buffers.allocated:
        enabled: true
      postgresql.bgwriter.buffers.writes:
        enabled: true
      postgresql.bgwriter.checkpoint.count:
        enabled: true
      postgresql.bgwriter.duration:
        enabled: true
      postgresql.bgwriter.maxwritten:
        enabled: true
      postgresql.blocks:
        enabled: true
      postgresql.commits:
        enabled: true
      postgresql.connection.max:
        enabled: true
      postgresql.connection.count:
        enabled: true
      postgresql.database.count:
        enabled: true
      postgresql.database.size:
        enabled: true
      postgresql.db_size:
        enabled: true
      postgresql.deadlocks:
        enabled: true
      postgresql.index.scans:
        enabled: true
      postgresql.index.size:
        enabled: true
      postgresql.operations:
        enabled: true
      postgresql.replication.data_delay:
        enabled: true
      postgresql.rollbacks:
        enabled: true
      postgresql.rows:
        enabled: true
      postgresql.sequential_scans:
        enabled: true
      postgresql.table.count:
        enabled: true
      postgresql.table.size:
        enabled: true
      postgresql.table.vacuum.count:
        enabled: true
      postgresql.temp_files:
        enabled: true
      postgresql.wal.age:
        enabled: true
      postgresql.wal.lag:
        enabled: true

  # Host metrics receiver
  hostmetrics:
    root_path: /
    collection_interval: 30s
    scrapers:
      cpu:
        metrics:
          system.cpu.utilization:
            enabled: true
      memory:
        metrics:
          system.memory.utilization:
            enabled: true
          system.memory.usage:
            enabled: true
      disk:
        metrics:
          system.disk.io:
            enabled: true
          system.disk.io_time:
            enabled: true
          system.disk.operations:
            enabled: true
      filesystem:
        metrics:
          system.filesystem.utilization:
            enabled: true
      network:
        metrics:
          system.network.io:
            enabled: true
          system.network.errors:
            enabled: true
          system.network.dropped:
            enabled: true
      load:
        metrics:
          system.cpu.load_average.1m:
            enabled: true
          system.cpu.load_average.5m:
            enabled: true
          system.cpu.load_average.15m:
            enabled: true

  # SQL query receiver for custom metrics
  sqlquery:
    driver: postgres
    datasource: "host=${DB_HOST} port=${DB_PORT} user=${DB_USERNAME} password=${DB_PASSWORD} sslmode=require"
    collection_interval: 60s
    queries:
      # Active connections by state
      - sql: |
          SELECT 
            state,
            COUNT(*) as connection_count
          FROM pg_stat_activity
          GROUP BY state
        metrics:
          - metric_name: postgresql.connections.by_state
            value_column: connection_count
            value_type: gauge
            attribute_columns:
              - state

      # Long running queries
      - sql: |
          SELECT 
            COUNT(*) FILTER (WHERE extract(epoch from (now() - query_start)) > 60) as long_queries,
            MAX(extract(epoch from (now() - query_start))) as max_duration
          FROM pg_stat_activity 
          WHERE state = 'active' AND query NOT LIKE '%pg_stat_activity%'
        metrics:
          - metric_name: postgresql.queries.long_running.count
            value_column: long_queries
            value_type: gauge
          - metric_name: postgresql.queries.duration.max
            value_column: max_duration
            value_type: gauge
            unit: s

      # Table bloat estimation
      - sql: |
          SELECT 
            schemaname,
            tablename,
            pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size,
            pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
          FROM pg_tables 
          WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
          ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
          LIMIT 10
        metrics:
          - metric_name: postgresql.table.size.top10
            value_column: size_bytes
            value_type: gauge
            unit: By
            attribute_columns:
              - schemaname
              - tablename

processors:
  # Memory limiter to prevent OOM
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  # Resource processor to add metadata
  resource:
    attributes:
      - key: service.name
        value: "${SERVICE_NAME}"
        action: upsert
      - key: deployment.environment
        value: "${ENVIRONMENT}"
        action: upsert
      - key: db.system
        value: "postgresql"
        action: insert
      - key: instrumentation.provider
        value: "opentelemetry"
        action: insert
      - key: telemetry.sdk.name
        value: "opentelemetry"
        action: insert
      - key: telemetry.sdk.language
        value: "go"
        action: insert

  # Transform processor for metric adjustments
  transform:
    error_mode: ignore
    metric_statements:
      # Ensure correct units
      - context: metric
        statements:
          - set(unit, "By") where name == "postgresql.database.size"
          - set(unit, "By") where name == "postgresql.table.size"
          - set(unit, "1") where name == "postgresql.connections.active"

  # Attributes processor to manage cardinality
  attributes:
    actions:
      # Remove high-cardinality query text
      - key: query.text
        action: delete
      # Remove internal usernames
      - key: user.name
        pattern: ^(postgres|pg_|repl).*
        action: delete
      # Normalize database names
      - key: db.name
        action: hash

  # Filter processor to drop unwanted metrics
  filter:
    error_mode: ignore
    metrics:
      # Drop metrics from system databases
      datapoint:
        - 'attributes["db.name"] == "template0"'
        - 'attributes["db.name"] == "template1"'

  # Convert cumulative counters to delta for New Relic
  cumulativetodelta:
    include:
      match_type: regexp
      metric_names:
        - "postgresql.commits"
        - "postgresql.rollbacks"
        - "postgresql.blocks.*"
        - "postgresql.rows.*"
        - "postgresql.operations.*"
        - "postgresql.bgwriter.*"

  # Batch processor for efficiency
  batch:
    timeout: 10s
    send_batch_size: 1024
    send_batch_max_size: 2048

exporters:
  # OTLP exporter to New Relic
  otlp:
    endpoint: "${NEW_RELIC_OTLP_ENDPOINT}"
    headers:
      api-key: "${NEW_RELIC_LICENSE_KEY}"
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 1000
      storage: file_storage

  # Debug exporter (comment out in production)
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 100

extensions:
  # Health check extension
  health_check:
    endpoint: "0.0.0.0:13133"
    path: "/health"

  # Performance profiler (optional)
  pprof:
    endpoint: "0.0.0.0:1777"

  # File storage for queue persistence
  file_storage:
    directory: /var/lib/otelcol/file_storage
    timeout: 10s

service:
  extensions: [health_check, pprof, file_storage]
  
  pipelines:
    metrics:
      receivers:
        - postgresql
        - hostmetrics
        - sqlquery
      processors:
        - memory_limiter
        - resource
        - transform
        - attributes
        - filter
        - cumulativetodelta
        - batch
      exporters:
        - otlp
        # - debug  # Uncomment for troubleshooting

  telemetry:
    logs:
      level: info
      initial_fields:
        service: otel-collector
    metrics:
      address: 0.0.0.0:8888
      level: detailed