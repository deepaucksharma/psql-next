# PostgreSQL Maximum Metrics Extraction - Config Only Approach
# This configuration extracts the maximum possible metrics using only stock OpenTelemetry components
# No custom code required - everything is achieved through configuration

receivers:
  # ============================================
  # CORE POSTGRESQL RECEIVER
  # ============================================
  postgresql:
    endpoint: ${env:POSTGRES_HOST:localhost}:${env:POSTGRES_PORT:5432}
    transport: tcp
    username: ${env:POSTGRES_USER:postgres}
    password: ${env:POSTGRES_PASSWORD}
    databases:
      - ${env:POSTGRES_DB:postgres}
    collection_interval: 10s
    tls:
      insecure: true
      insecure_skip_verify: true
    # Enable all available metrics
    metrics:
      postgresql.backends:
        enabled: true
      postgresql.bgwriter.buffers.allocated:
        enabled: true
      postgresql.bgwriter.buffers.writes:
        enabled: true
      postgresql.bgwriter.checkpoint.count:
        enabled: true
      postgresql.bgwriter.duration:
        enabled: true
      postgresql.bgwriter.maxwritten:
        enabled: true
      postgresql.blocks_read:
        enabled: true
      postgresql.commits:
        enabled: true
      postgresql.connection.max:
        enabled: true
      postgresql.database.count:
        enabled: true
      postgresql.db_size:
        enabled: true
      postgresql.deadlocks:
        enabled: true
      postgresql.index.scans:
        enabled: true
      postgresql.index.size:
        enabled: true
      postgresql.operations:
        enabled: true
      postgresql.replication.data_delay:
        enabled: true
      postgresql.rollbacks:
        enabled: true
      postgresql.rows:
        enabled: true
      postgresql.table.count:
        enabled: true
      postgresql.table.size:
        enabled: true
      postgresql.table.vacuum.count:
        enabled: true
      postgresql.temp_files:
        enabled: true
      postgresql.wal.age:
        enabled: true
      postgresql.wal.delay:
        enabled: true
      postgresql.wal.lag:
        enabled: true

  # ============================================
  # ACTIVE SESSION HISTORY (ASH) SIMULATION
  # ============================================
  sqlquery/ash:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:localhost} port=${env:POSTGRES_PORT:5432} user=${env:POSTGRES_USER:postgres} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB:postgres} sslmode=disable"
    collection_interval: 1s  # High frequency for ASH-like sampling
    queries:
      - sql: |
          SELECT 
            COALESCE(state, 'unknown') as session_state,
            COALESCE(wait_event_type, 'CPU') as wait_event_type,
            COALESCE(wait_event, 'CPU') as wait_event,
            COALESCE(backend_type, 'unknown') as backend_type,
            COUNT(*) as session_count,
            COUNT(*) FILTER (WHERE query_start < NOW() - INTERVAL '10 seconds') as long_running_count,
            MAX(EXTRACT(EPOCH FROM (NOW() - query_start))) as max_query_duration_seconds,
            AVG(EXTRACT(EPOCH FROM (NOW() - query_start))) as avg_query_duration_seconds
          FROM pg_stat_activity
          WHERE pid != pg_backend_pid()
          GROUP BY state, wait_event_type, wait_event, backend_type
        metrics:
          - metric_name: postgresql.ash.sessions
            value_column: session_count
            value_type: gauge
            unit: "{sessions}"
            attribute_columns: [session_state, wait_event_type, wait_event, backend_type]
          - metric_name: postgresql.ash.sessions.long_running
            value_column: long_running_count
            value_type: gauge
            unit: "{sessions}"
            attribute_columns: [session_state, wait_event_type, wait_event, backend_type]
          - metric_name: postgresql.ash.query_duration.max
            value_column: max_query_duration_seconds
            value_type: gauge
            unit: s
            attribute_columns: [session_state, wait_event_type, wait_event, backend_type]
          - metric_name: postgresql.ash.query_duration.avg
            value_column: avg_query_duration_seconds
            value_type: gauge
            unit: s
            attribute_columns: [session_state, wait_event_type, wait_event, backend_type]

  # ============================================
  # QUERY PERFORMANCE METRICS
  # ============================================
  sqlquery/query_stats:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:localhost} port=${env:POSTGRES_PORT:5432} user=${env:POSTGRES_USER:postgres} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB:postgres} sslmode=disable"
    collection_interval: 30s
    queries:
      # Top queries by total time
      - sql: |
          SELECT 
            queryid::text as query_id,
            LEFT(query, 50) as query_text_sample,
            calls,
            total_exec_time,
            mean_exec_time,
            stddev_exec_time,
            min_exec_time,
            max_exec_time,
            rows,
            100.0 * shared_blks_hit / NULLIF(shared_blks_hit + shared_blks_read, 0) AS cache_hit_ratio,
            shared_blks_read,
            shared_blks_written,
            local_blks_read,
            local_blks_written,
            temp_blks_read,
            temp_blks_written,
            blk_read_time,
            blk_write_time,
            wal_records,
            wal_fpi,
            wal_bytes
          FROM pg_stat_statements
          WHERE calls > 5
          ORDER BY total_exec_time DESC
          LIMIT 50
        metrics:
          - metric_name: postgresql.query.calls
            value_column: calls
            value_type: gauge
            unit: "{calls}"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.total_time
            value_column: total_exec_time
            value_type: gauge
            unit: ms
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.mean_time
            value_column: mean_exec_time
            value_type: gauge
            unit: ms
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.stddev_time
            value_column: stddev_exec_time
            value_type: gauge
            unit: ms
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.min_time
            value_column: min_exec_time
            value_type: gauge
            unit: ms
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.max_time
            value_column: max_exec_time
            value_type: gauge
            unit: ms
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.rows
            value_column: rows
            value_type: gauge
            unit: "{rows}"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.cache_hit_ratio
            value_column: cache_hit_ratio
            value_type: gauge
            unit: "%"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.shared_blocks_read
            value_column: shared_blks_read
            value_type: gauge
            unit: "{blocks}"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.shared_blocks_written
            value_column: shared_blks_written
            value_type: gauge
            unit: "{blocks}"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.temp_blocks_read
            value_column: temp_blks_read
            value_type: gauge
            unit: "{blocks}"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.temp_blocks_written
            value_column: temp_blks_written
            value_type: gauge
            unit: "{blocks}"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.blk_read_time
            value_column: blk_read_time
            value_type: gauge
            unit: ms
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.blk_write_time
            value_column: blk_write_time
            value_type: gauge
            unit: ms
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.wal_records
            value_column: wal_records
            value_type: gauge
            unit: "{records}"
            attribute_columns: [query_id, query_text_sample]
          - metric_name: postgresql.query.wal_bytes
            value_column: wal_bytes
            value_type: gauge
            unit: By
            attribute_columns: [query_id, query_text_sample]

  # ============================================
  # BLOCKING AND LOCK ANALYSIS
  # ============================================
  sqlquery/blocking:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:localhost} port=${env:POSTGRES_PORT:5432} user=${env:POSTGRES_USER:postgres} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB:postgres} sslmode=disable"
    collection_interval: 5s
    queries:
      - sql: |
          WITH blocking_tree AS (
            SELECT 
              blocked.pid AS blocked_pid,
              blocked.usename AS blocked_user,
              blocked.query AS blocked_query,
              blocked.state AS blocked_state,
              blocking.pid AS blocking_pid,
              blocking.usename AS blocking_user,
              blocking.query AS blocking_query,
              blocking.state AS blocking_state,
              EXTRACT(EPOCH FROM (NOW() - blocked.query_start)) AS blocked_duration_seconds
            FROM pg_stat_activity AS blocked
            JOIN pg_stat_activity AS blocking ON blocking.pid = ANY(pg_blocking_pids(blocked.pid))
            WHERE blocked.pid != pg_backend_pid()
          )
          SELECT 
            COUNT(*) as blocking_chain_count,
            MAX(blocked_duration_seconds) as max_blocked_duration,
            COUNT(DISTINCT blocked_pid) as unique_blocked_sessions,
            COUNT(DISTINCT blocking_pid) as unique_blocking_sessions
          FROM blocking_tree
        metrics:
          - metric_name: postgresql.blocking.chain_count
            value_column: blocking_chain_count
            value_type: gauge
            unit: "{chains}"
          - metric_name: postgresql.blocking.max_duration
            value_column: max_blocked_duration
            value_type: gauge
            unit: s
          - metric_name: postgresql.blocking.blocked_sessions
            value_column: unique_blocked_sessions
            value_type: gauge
            unit: "{sessions}"
          - metric_name: postgresql.blocking.blocking_sessions
            value_column: unique_blocking_sessions
            value_type: gauge
            unit: "{sessions}"

      # Lock wait analysis
      - sql: |
          SELECT 
            locktype,
            mode,
            COUNT(*) as lock_count,
            COUNT(*) FILTER (WHERE granted = false) as waiting_lock_count,
            COUNT(DISTINCT pid) as session_count
          FROM pg_locks
          GROUP BY locktype, mode
        metrics:
          - metric_name: postgresql.locks.count
            value_column: lock_count
            value_type: gauge
            unit: "{locks}"
            attribute_columns: [locktype, mode]
          - metric_name: postgresql.locks.waiting
            value_column: waiting_lock_count
            value_type: gauge
            unit: "{locks}"
            attribute_columns: [locktype, mode]
          - metric_name: postgresql.locks.sessions
            value_column: session_count
            value_type: gauge
            unit: "{sessions}"
            attribute_columns: [locktype, mode]

  # ============================================
  # TABLE AND INDEX ANALYTICS
  # ============================================
  sqlquery/table_stats:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:localhost} port=${env:POSTGRES_PORT:5432} user=${env:POSTGRES_USER:postgres} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB:postgres} sslmode=disable"
    collection_interval: 60s
    queries:
      # Table access patterns
      - sql: |
          SELECT 
            schemaname,
            tablename,
            seq_scan,
            seq_tup_read,
            idx_scan,
            idx_tup_fetch,
            n_tup_ins,
            n_tup_upd,
            n_tup_del,
            n_tup_hot_upd,
            n_live_tup,
            n_dead_tup,
            CASE WHEN n_live_tup > 0 
              THEN (n_dead_tup::float / n_live_tup::float) * 100 
              ELSE 0 
            END as dead_tuple_ratio,
            last_vacuum,
            last_autovacuum,
            last_analyze,
            last_autoanalyze,
            vacuum_count,
            autovacuum_count,
            analyze_count,
            autoanalyze_count
          FROM pg_stat_user_tables
          WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
        metrics:
          - metric_name: postgresql.table.seq_scan
            value_column: seq_scan
            value_type: gauge
            unit: "{scans}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.seq_tup_read
            value_column: seq_tup_read
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.idx_scan
            value_column: idx_scan
            value_type: gauge
            unit: "{scans}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.idx_tup_fetch
            value_column: idx_tup_fetch
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.n_tup_ins
            value_column: n_tup_ins
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.n_tup_upd
            value_column: n_tup_upd
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.n_tup_del
            value_column: n_tup_del
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.n_tup_hot_upd
            value_column: n_tup_hot_upd
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.n_live_tup
            value_column: n_live_tup
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.n_dead_tup
            value_column: n_dead_tup
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.dead_tuple_ratio
            value_column: dead_tuple_ratio
            value_type: gauge
            unit: "%"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.vacuum_count
            value_column: vacuum_count
            value_type: gauge
            unit: "{vacuums}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.table.autovacuum_count
            value_column: autovacuum_count
            value_type: gauge
            unit: "{vacuums}"
            attribute_columns: [schemaname, tablename]

      # Index efficiency
      - sql: |
          SELECT 
            schemaname,
            tablename,
            indexname,
            idx_scan,
            idx_tup_read,
            idx_tup_fetch,
            CASE WHEN idx_scan > 0 
              THEN ROUND((idx_tup_fetch::float / idx_scan::float)::numeric, 2)
              ELSE 0 
            END as avg_tuples_per_scan
          FROM pg_stat_user_indexes
          WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
        metrics:
          - metric_name: postgresql.index.scan_count
            value_column: idx_scan
            value_type: gauge
            unit: "{scans}"
            attribute_columns: [schemaname, tablename, indexname]
          - metric_name: postgresql.index.tup_read
            value_column: idx_tup_read
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename, indexname]
          - metric_name: postgresql.index.tup_fetch
            value_column: idx_tup_fetch
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename, indexname]
          - metric_name: postgresql.index.avg_tuples_per_scan
            value_column: avg_tuples_per_scan
            value_type: gauge
            unit: "{tuples/scan}"
            attribute_columns: [schemaname, tablename, indexname]

  # ============================================
  # DATABASE HEALTH METRICS
  # ============================================
  sqlquery/health:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:localhost} port=${env:POSTGRES_PORT:5432} user=${env:POSTGRES_USER:postgres} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB:postgres} sslmode=disable"
    collection_interval: 30s
    queries:
      # Connection health
      - sql: |
          SELECT 
            datname,
            numbackends,
            xact_commit,
            xact_rollback,
            blks_read,
            blks_hit,
            CASE WHEN blks_read + blks_hit > 0 
              THEN ROUND((blks_hit::float / (blks_read + blks_hit)::float * 100)::numeric, 2)
              ELSE 100 
            END as cache_hit_ratio,
            tup_returned,
            tup_fetched,
            tup_inserted,
            tup_updated,
            tup_deleted,
            conflicts,
            temp_files,
            temp_bytes,
            deadlocks,
            checksum_failures,
            checksum_last_failure,
            blk_read_time,
            blk_write_time,
            session_time,
            active_time,
            idle_in_transaction_time,
            sessions,
            sessions_abandoned,
            sessions_fatal,
            sessions_killed
          FROM pg_stat_database
          WHERE datname NOT IN ('template0', 'template1')
        metrics:
          - metric_name: postgresql.database.numbackends
            value_column: numbackends
            value_type: gauge
            unit: "{backends}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.xact_commit
            value_column: xact_commit
            value_type: gauge
            unit: "{transactions}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.xact_rollback
            value_column: xact_rollback
            value_type: gauge
            unit: "{transactions}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.blks_read
            value_column: blks_read
            value_type: gauge
            unit: "{blocks}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.blks_hit
            value_column: blks_hit
            value_type: gauge
            unit: "{blocks}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.cache_hit_ratio
            value_column: cache_hit_ratio
            value_type: gauge
            unit: "%"
            attribute_columns: [datname]
          - metric_name: postgresql.database.tup_returned
            value_column: tup_returned
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.tup_fetched
            value_column: tup_fetched
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.tup_inserted
            value_column: tup_inserted
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.tup_updated
            value_column: tup_updated
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.tup_deleted
            value_column: tup_deleted
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.conflicts
            value_column: conflicts
            value_type: gauge
            unit: "{conflicts}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.temp_files
            value_column: temp_files
            value_type: gauge
            unit: "{files}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.temp_bytes
            value_column: temp_bytes
            value_type: gauge
            unit: By
            attribute_columns: [datname]
          - metric_name: postgresql.database.deadlocks
            value_column: deadlocks
            value_type: gauge
            unit: "{deadlocks}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.blk_read_time
            value_column: blk_read_time
            value_type: gauge
            unit: ms
            attribute_columns: [datname]
          - metric_name: postgresql.database.blk_write_time
            value_column: blk_write_time
            value_type: gauge
            unit: ms
            attribute_columns: [datname]
          - metric_name: postgresql.database.sessions
            value_column: sessions
            value_type: gauge
            unit: "{sessions}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.sessions_abandoned
            value_column: sessions_abandoned
            value_type: gauge
            unit: "{sessions}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.sessions_fatal
            value_column: sessions_fatal
            value_type: gauge
            unit: "{sessions}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.sessions_killed
            value_column: sessions_killed
            value_type: gauge
            unit: "{sessions}"
            attribute_columns: [datname]

      # Checkpoint performance
      - sql: |
          SELECT 
            checkpoints_timed,
            checkpoints_req,
            checkpoint_write_time,
            checkpoint_sync_time,
            buffers_checkpoint,
            buffers_clean,
            maxwritten_clean,
            buffers_backend,
            buffers_backend_fsync,
            buffers_alloc
          FROM pg_stat_bgwriter
        metrics:
          - metric_name: postgresql.bgwriter.checkpoints_timed
            value_column: checkpoints_timed
            value_type: gauge
            unit: "{checkpoints}"
          - metric_name: postgresql.bgwriter.checkpoints_req
            value_column: checkpoints_req
            value_type: gauge
            unit: "{checkpoints}"
          - metric_name: postgresql.bgwriter.checkpoint_write_time
            value_column: checkpoint_write_time
            value_type: gauge
            unit: ms
          - metric_name: postgresql.bgwriter.checkpoint_sync_time
            value_column: checkpoint_sync_time
            value_type: gauge
            unit: ms
          - metric_name: postgresql.bgwriter.buffers_checkpoint
            value_column: buffers_checkpoint
            value_type: gauge
            unit: "{buffers}"
          - metric_name: postgresql.bgwriter.buffers_clean
            value_column: buffers_clean
            value_type: gauge
            unit: "{buffers}"
          - metric_name: postgresql.bgwriter.maxwritten_clean
            value_column: maxwritten_clean
            value_type: gauge
            unit: "{times}"
          - metric_name: postgresql.bgwriter.buffers_backend
            value_column: buffers_backend
            value_type: gauge
            unit: "{buffers}"
          - metric_name: postgresql.bgwriter.buffers_backend_fsync
            value_column: buffers_backend_fsync
            value_type: gauge
            unit: "{buffers}"
          - metric_name: postgresql.bgwriter.buffers_alloc
            value_column: buffers_alloc
            value_type: gauge
            unit: "{buffers}"

  # ============================================
  # REPLICATION MONITORING
  # ============================================
  sqlquery/replication:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:localhost} port=${env:POSTGRES_PORT:5432} user=${env:POSTGRES_USER:postgres} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB:postgres} sslmode=disable"
    collection_interval: 10s
    queries:
      # Replication lag
      - sql: |
          SELECT 
            client_addr::text,
            application_name,
            state,
            sync_state,
            EXTRACT(EPOCH FROM (NOW() - backend_start)) as connection_age_seconds,
            CASE WHEN pg_is_in_recovery() 
              THEN 0 
              ELSE EXTRACT(EPOCH FROM (NOW() - pg_last_xact_replay_timestamp()))::int 
            END as replication_lag_seconds,
            CASE WHEN state = 'streaming' 
              THEN COALESCE(pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn), 0)
              ELSE 0 
            END as replication_lag_bytes
          FROM pg_stat_replication
        metrics:
          - metric_name: postgresql.replication.connection_age
            value_column: connection_age_seconds
            value_type: gauge
            unit: s
            attribute_columns: [client_addr, application_name, state, sync_state]
          - metric_name: postgresql.replication.lag_time
            value_column: replication_lag_seconds
            value_type: gauge
            unit: s
            attribute_columns: [client_addr, application_name, state, sync_state]
          - metric_name: postgresql.replication.lag_bytes
            value_column: replication_lag_bytes
            value_type: gauge
            unit: By
            attribute_columns: [client_addr, application_name, state, sync_state]

      # Replication slots
      - sql: |
          SELECT 
            slot_name,
            slot_type,
            active,
            CASE WHEN NOT pg_is_in_recovery() 
              THEN pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn) 
              ELSE 0 
            END as retained_wal_bytes
          FROM pg_replication_slots
        metrics:
          - metric_name: postgresql.replication_slot.retained_wal_bytes
            value_column: retained_wal_bytes
            value_type: gauge
            unit: By
            attribute_columns: [slot_name, slot_type, active]

  # ============================================
  # ADVANCED PERFORMANCE METRICS
  # ============================================
  sqlquery/performance:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:localhost} port=${env:POSTGRES_PORT:5432} user=${env:POSTGRES_USER:postgres} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB:postgres} sslmode=disable"
    collection_interval: 30s
    queries:
      # Buffer cache statistics
      - sql: |
          SELECT 
            c.relname,
            pg_size_pretty(count(*) * 8192) as buffered_size,
            round(100.0 * count(*) / NULLIF((SELECT setting FROM pg_settings WHERE name='shared_buffers')::integer, 0), 2) as buffer_percent,
            round(100.0 * count(*) * 8192 / NULLIF(pg_table_size(c.oid), 0), 2) as percent_of_table
          FROM pg_buffercache b
          INNER JOIN pg_class c ON b.relfilenode = c.relfilenode
          WHERE c.relkind IN ('r', 'i', 'm')
          GROUP BY c.oid, c.relname
          ORDER BY count(*) DESC
          LIMIT 20
        metrics:
          - metric_name: postgresql.buffer_cache.percent_used
            value_column: buffer_percent
            value_type: gauge
            unit: "%"
            attribute_columns: [relname]
          - metric_name: postgresql.buffer_cache.table_percent_cached
            value_column: percent_of_table
            value_type: gauge
            unit: "%"
            attribute_columns: [relname]

      # Transaction ID wraparound monitoring
      - sql: |
          SELECT 
            datname,
            age(datfrozenxid) as database_age,
            2147483647 - age(datfrozenxid) as transactions_until_wraparound,
            ROUND(100.0 * age(datfrozenxid) / 2147483647, 2) as wraparound_percent
          FROM pg_database
          WHERE datname NOT IN ('template0', 'template1')
          ORDER BY age(datfrozenxid) DESC
        metrics:
          - metric_name: postgresql.database.transaction_id_age
            value_column: database_age
            value_type: gauge
            unit: "{transactions}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.transactions_until_wraparound
            value_column: transactions_until_wraparound
            value_type: gauge
            unit: "{transactions}"
            attribute_columns: [datname]
          - metric_name: postgresql.database.wraparound_percent
            value_column: wraparound_percent
            value_type: gauge
            unit: "%"
            attribute_columns: [datname]

      # Autovacuum effectiveness
      - sql: |
          WITH table_stats AS (
            SELECT 
              schemaname,
              tablename,
              n_dead_tup,
              n_live_tup,
              last_autovacuum,
              EXTRACT(EPOCH FROM (NOW() - last_autovacuum)) as seconds_since_autovacuum
            FROM pg_stat_user_tables
            WHERE n_live_tup > 0
          )
          SELECT 
            schemaname,
            tablename,
            n_dead_tup,
            n_live_tup,
            ROUND((n_dead_tup::float / n_live_tup::float * 100)::numeric, 2) as dead_tuple_percent,
            seconds_since_autovacuum
          FROM table_stats
          WHERE n_dead_tup > 1000
          ORDER BY dead_tuple_percent DESC
          LIMIT 20
        metrics:
          - metric_name: postgresql.autovacuum.dead_tuples
            value_column: n_dead_tup
            value_type: gauge
            unit: "{tuples}"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.autovacuum.dead_tuple_percent
            value_column: dead_tuple_percent
            value_type: gauge
            unit: "%"
            attribute_columns: [schemaname, tablename]
          - metric_name: postgresql.autovacuum.seconds_since_last
            value_column: seconds_since_autovacuum
            value_type: gauge
            unit: s
            attribute_columns: [schemaname, tablename]

  # ============================================
  # HOST METRICS RECEIVER
  # ============================================
  hostmetrics:
    collection_interval: 10s
    scrapers:
      cpu:
        metrics:
          system.cpu.utilization:
            enabled: true
      memory:
        metrics:
          system.memory.utilization:
            enabled: true
      disk:
        metrics:
          system.disk.io:
            enabled: true
          system.disk.operations:
            enabled: true
          system.disk.io_time:
            enabled: true
          system.disk.operation_time:
            enabled: true
      filesystem:
        metrics:
          system.filesystem.utilization:
            enabled: true
      network:
        metrics:
          system.network.io:
            enabled: true
          system.network.errors:
            enabled: true
          system.network.dropped:
            enabled: true
      load:
        metrics:
          system.cpu.load_average.1m:
            enabled: true
          system.cpu.load_average.5m:
            enabled: true
          system.cpu.load_average.15m:
            enabled: true
      processes:
      paging:
        metrics:
          system.paging.usage:
            enabled: true
          system.paging.operations:
            enabled: true

# ============================================
# PROCESSORS
# ============================================
processors:
  # Add resource attributes
  resource:
    attributes:
      - key: service.name
        value: database-intelligence
        action: insert
      - key: deployment.environment
        value: ${env:ENVIRONMENT:production}
        action: insert
      - key: db.system
        value: postgresql
        action: insert
      - key: deployment.mode
        value: config-only-maximum
        action: insert

  # Add database connection info
  resource/detection:
    detectors: [env, system]
    timeout: 2s
    override: false
    system:
      hostname_sources: ["os"]
      resource_attributes:
        host.name:
          enabled: true
        host.id:
          enabled: true

  # Memory limiter
  memory_limiter:
    check_interval: 1s
    limit_mib: 1024
    spike_limit_mib: 256

  # Batch processing
  batch:
    timeout: 10s
    send_batch_size: 1000
    send_batch_max_size: 1500

  # Transform metrics to add custom attributes
  transform/add_metadata:
    metric_statements:
      - context: datapoint
        statements:
          # Add query classification for performance metrics
          - set(attributes["query.classification"], "slow") where attributes["mean_exec_time"] != nil and attributes["mean_exec_time"] > 1000
          - set(attributes["query.classification"], "medium") where attributes["mean_exec_time"] != nil and attributes["mean_exec_time"] > 100 and attributes["mean_exec_time"] <= 1000
          - set(attributes["query.classification"], "fast") where attributes["mean_exec_time"] != nil and attributes["mean_exec_time"] <= 100
          
          # Add session classification
          - set(attributes["session.classification"], "long_running") where attributes["max_query_duration_seconds"] != nil and attributes["max_query_duration_seconds"] > 300
          - set(attributes["session.classification"], "normal") where attributes["max_query_duration_seconds"] != nil and attributes["max_query_duration_seconds"] <= 300
          
          # Add replication health status
          - set(attributes["replication.health"], "critical") where attributes["replication_lag_seconds"] != nil and attributes["replication_lag_seconds"] > 300
          - set(attributes["replication.health"], "warning") where attributes["replication_lag_seconds"] != nil and attributes["replication_lag_seconds"] > 60 and attributes["replication_lag_seconds"] <= 300
          - set(attributes["replication.health"], "healthy") where attributes["replication_lag_seconds"] != nil and attributes["replication_lag_seconds"] <= 60

  # Filter out noisy metrics if needed
  filter/reduce_cardinality:
    metrics:
      exclude:
        match_type: regexp
        metric_names:
          # Uncomment to exclude per-query metrics if cardinality is too high
          # - "postgresql\\.query\\..*"
          # Uncomment to exclude per-table metrics if needed
          # - "postgresql\\.table\\..*"
          # - "postgresql\\.index\\..*"

  # Metric grouping for aggregation
  groupbyattrs/aggregate:
    keys:
      - db.system
      - deployment.environment
      - service.name

  # Add k8s attributes if running in kubernetes
  k8sattributes:
    auth_type: "serviceAccount"
    passthrough: false
    extract:
      metadata:
        - k8s.namespace.name
        - k8s.deployment.name
        - k8s.statefulset.name
        - k8s.daemonset.name
        - k8s.cronjob.name
        - k8s.job.name
        - k8s.node.name
        - k8s.pod.name
        - k8s.pod.uid
        - k8s.pod.start_time
    pod_association:
      - sources:
          - from: resource_attribute
            name: k8s.pod.ip
      - sources:
          - from: resource_attribute
            name: k8s.pod.uid
      - sources:
          - from: connection

# ============================================
# EXPORTERS
# ============================================
exporters:
  # Debug exporter for troubleshooting
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 200

  # Prometheus exporter for local metrics
  prometheus:
    endpoint: "0.0.0.0:8888"
    namespace: db_intel
    const_labels:
      deployment.mode: config_only_maximum

  # Primary New Relic exporter
  otlp/newrelic:
    endpoint: ${env:NEW_RELIC_OTLP_ENDPOINT:otlp.nr-data.net:4317}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000

  # File exporter for backup/analysis
  file/backup:
    path: /var/log/otel/metrics.json
    rotation:
      max_megabytes: 100
      max_days: 7
      max_backups: 3

# ============================================
# EXTENSIONS
# ============================================
extensions:
  # Health check
  health_check:
    endpoint: 0.0.0.0:13133
    check_collector_pipeline:
      enabled: true
      interval: 5s
      exporter_failure_threshold: 5

  # Performance profiling
  pprof:
    endpoint: 0.0.0.0:1777
    save_to_file: /var/log/otel/profiles/

  # zPages for debugging
  zpages:
    endpoint: 0.0.0.0:55679

# ============================================
# SERVICE
# ============================================
service:
  extensions: [health_check, pprof, zpages]
  
  pipelines:
    # High-frequency metrics (ASH, current activity)
    metrics/high_frequency:
      receivers: [sqlquery/ash]
      processors: [memory_limiter, resource, resource/detection, transform/add_metadata, batch]
      exporters: [otlp/newrelic, prometheus]

    # Standard metrics
    metrics/standard:
      receivers: [postgresql, sqlquery/health, sqlquery/blocking, sqlquery/replication]
      processors: [memory_limiter, resource, resource/detection, batch]
      exporters: [otlp/newrelic, prometheus]

    # Performance metrics (lower frequency)
    metrics/performance:
      receivers: [sqlquery/query_stats, sqlquery/table_stats, sqlquery/performance]
      processors: [memory_limiter, resource, resource/detection, transform/add_metadata, filter/reduce_cardinality, batch]
      exporters: [otlp/newrelic, file/backup]

    # Host metrics
    metrics/host:
      receivers: [hostmetrics]
      processors: [memory_limiter, resource, batch]
      exporters: [otlp/newrelic]

  telemetry:
    logs:
      level: ${env:LOG_LEVEL:info}
      output_paths: ["stdout", "/var/log/otel/collector.log"]
    metrics:
      level: detailed
      address: 0.0.0.0:8889