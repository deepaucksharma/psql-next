# Production Configuration for Database Intelligence Collector
# This configuration demonstrates all integrated features:
# - Connection pooling for efficiency
# - Health monitoring endpoints
# - Rate limiting for protection
# - Secrets management for security
# - Circuit breakers for reliability
# - Adaptive sampling for cost control

receivers:
  # PostgreSQL receiver with standard metrics
  postgresql:
    endpoint: ${secret:POSTGRES_HOST}:${secret:POSTGRES_PORT}
    username: ${secret:POSTGRES_USER}
    password: ${secret:POSTGRES_PASSWORD}
    databases:
      - ${secret:POSTGRES_DATABASE}
    collection_interval: 60s
    tls:
      insecure: true

  # Enhanced SQL receiver with connection pooling and feature detection
  enhancedsql:
    driver: postgres
    datasource: postgres://${secret:POSTGRES_USER}:${secret:POSTGRES_PASSWORD}@${secret:POSTGRES_HOST}:${secret:POSTGRES_PORT}/${secret:POSTGRES_DATABASE}?sslmode=disable
    collection_interval: 30s
    # Connection pooling configuration
    max_open_connections: 25
    max_idle_connections: 10
    # Feature detection for intelligent query selection
    feature_detection:
      enabled: true
      cache_duration: 10m
      refresh_interval: 1h
    queries:
      - name: slow_queries
        category: slow_queries
        timeout: 30s
        max_rows: 100
      - name: active_sessions
        category: active_sessions
        timeout: 10s
      - name: wait_events
        category: wait_events
        timeout: 10s

  # SQL query receiver for OHI compatibility
  sqlquery/ohi_metrics:
    driver: postgres
    datasource: postgres://${secret:POSTGRES_USER}:${secret:POSTGRES_PASSWORD}@${secret:POSTGRES_HOST}:${secret:POSTGRES_PORT}/${secret:POSTGRES_DATABASE}?sslmode=disable
    queries:
      - sql: |
          SELECT 
            datname as database_name,
            numbackends as connection_count,
            xact_commit as transactions_committed,
            xact_rollback as transactions_rolled_back,
            blks_read as blocks_read,
            blks_hit as blocks_hit,
            tup_returned as tuples_returned,
            tup_fetched as tuples_fetched,
            tup_inserted as tuples_inserted,
            tup_updated as tuples_updated,
            tup_deleted as tuples_deleted
          FROM pg_stat_database
          WHERE datname = current_database()
        metrics:
          - metric_name: postgresql.database.connections
            value_column: connection_count
          - metric_name: postgresql.database.transactions.committed
            value_column: transactions_committed
          - metric_name: postgresql.database.transactions.rolled_back
            value_column: transactions_rolled_back
          - metric_name: postgresql.database.blocks.read
            value_column: blocks_read
          - metric_name: postgresql.database.blocks.hit
            value_column: blocks_hit
        collection_interval: 60s

processors:
  # Batch processor for efficiency
  batch:
    timeout: 10s
    send_batch_size: 1000
    send_batch_max_size: 2000

  # Memory limiter for protection
  memory_limiter:
    check_interval: 1s
    limit_mib: 1024
    spike_limit_mib: 256

  # Adaptive sampler for cost control
  adaptivesampler:
    enabled: true
    sampling_rules:
      - service_name: "database-*"
        initial_sampling_rate: 0.1
        target_rate: 0.5
        min_rate: 0.01
        max_rate: 1.0
        adjustment_period: 5m
    metrics_to_track:
      - postgresql.database.connections
      - postgresql.database.transactions.*

  # Circuit breaker for reliability
  circuitbreaker:
    failure_threshold: 5
    recovery_timeout: 30s
    half_open_requests: 3
    tracked_errors:
      - "connection refused"
      - "timeout"
      - "rate limit exceeded"

  # Cost control processor
  costcontrol:
    enabled: true
    max_data_points_per_minute: 100000
    enforcement_mode: drop  # or "sample"
    priority_metrics:
      - postgresql.database.connections
      - postgresql.slow_queries.count
      - postgresql.database.size

  # Plan attribute extractor for query analysis
  planattributeextractor:
    extract_query_plan: true
    extract_wait_events: true
    extract_io_timing: true
    cache_plans: true
    cache_size: 1000
    cache_ttl: 5m

  # Query correlator for holistic view
  querycorrelator:
    correlation_window: 5m
    min_correlation_score: 0.7
    max_queries_to_correlate: 100

  # New Relic error monitor
  nrerrormonitor:
    enabled: true
    api_key: ${secret:NEW_RELIC_API_KEY}
    account_id: ${secret:NEW_RELIC_ACCOUNT_ID}
    check_interval: 60s

  # Verification processor for data quality
  verification:
    enabled: true
    verify_timestamps: true
    verify_attributes: true
    required_attributes:
      - db.system
      - db.name
      - db.operation

exporters:
  # New Relic OTLP exporter
  otlphttp/newrelic:
    endpoint: https://otlp.nr-data.net:4318
    headers:
      api-key: ${secret:NEW_RELIC_API_KEY}
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 1000

  # NRI exporter with rate limiting
  nri:
    integration_name: com.newrelic.postgresql
    integration_version: 2.0.0
    protocol_version: 2
    entity:
      type: PostgreSQLInstance
      name_source: db.name
      display_name_template: "{{.db.name}} ({{.net.host.name}})"
      attributes:
        environment: ${secret:ENVIRONMENT}
        team: "database-ops"
    output:
      mode: http
      http_endpoint: https://metric-api.newrelic.com/metric/v1
      api_key: ${secret:NEW_RELIC_INGEST_KEY}
      batch_size: 1000
      flush_interval: 10s
    # Rate limiting configuration
    rate_limiting:
      enabled: true
      rps: 1000
      burst: 200
      global_max_rps: 5000
      global_max_burst: 1000
      enable_adaptive: true
      min_rps: 100
      max_rps: 2000
      database_limits:
        production:
          rps: 2000
          burst: 400
        staging:
          rps: 1000
          burst: 200
        development:
          rps: 500
          burst: 100
    metric_rules:
      - source_pattern: "postgresql.*"
        target_name: "db.{{.metric_suffix}}"
        nri_type: "GAUGE"
        scale_factor: 1.0
      - source_pattern: "postgres.slow_queries.*"
        target_name: "query.slow.{{.metric_suffix}}"
        nri_type: "RATE"
    event_rules:
      - source_pattern: "slow_query"
        event_type: "PostgresSlowQuery"
        category: "Performance"
        summary_template: "Slow query: {{.query}}"

  # Prometheus exporter for local monitoring
  prometheus:
    endpoint: 0.0.0.0:9090
    metric_expiration: 10m
    enable_open_metrics: true

  # Debug exporter (reduced verbosity in production)
  debug:
    verbosity: basic
    sampling_initial: 100
    sampling_thereafter: 10000

extensions:
  # Health check extension
  health_check:
    endpoint: 0.0.0.0:13133
    path: /health
    check_collector_pipeline:
      enabled: true
      interval: 15s
      exporter_failure_threshold: 5

  # Performance profiling
  zpages:
    endpoint: 0.0.0.0:55679

  # Memory ballast for GC optimization
  memory_ballast:
    size_mib: 512

service:
  # Extensions to enable
  extensions: [health_check, zpages, memory_ballast]

  # Pipeline definitions
  pipelines:
    # Main metrics pipeline
    metrics/primary:
      receivers: [postgresql, enhancedsql, sqlquery/ohi_metrics]
      processors: 
        - memory_limiter
        - batch
        - adaptivesampler
        - circuitbreaker
        - costcontrol
        - planattributeextractor
        - querycorrelator
        - verification
      exporters: [otlphttp/newrelic, nri, prometheus]

    # High-priority metrics pipeline (no sampling)
    metrics/critical:
      receivers: [postgresql]
      processors:
        - memory_limiter
        - batch
        - circuitbreaker
        - verification
      exporters: [otlphttp/newrelic]

    # Logs pipeline for query analysis
    logs:
      receivers: [enhancedsql]
      processors:
        - memory_limiter
        - batch
        - nrerrormonitor
      exporters: [nri]

  # Telemetry configuration
  telemetry:
    logs:
      level: info
      development: false
      encoding: json
      initial_fields:
        service: database-intelligence
        environment: ${secret:ENVIRONMENT}
    metrics:
      level: detailed
      address: 0.0.0.0:8888

# Production-specific settings
# - Higher collection intervals to reduce load
# - Stricter rate limits
# - Conservative sampling rates
# - Enhanced security with secrets
# - Multiple export paths for reliability