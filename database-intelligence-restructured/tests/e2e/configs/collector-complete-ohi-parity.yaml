# Complete OHI Parity Collector Configuration
# This configuration maps all PostgreSQL OHI events to OpenTelemetry metrics

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
    path: /health
  
  zpages:
    endpoint: 0.0.0.0:55679

receivers:
  # Standard PostgreSQL metrics
  postgresql:
    endpoint: localhost:5432
    username: postgres
    password: pass
    collection_interval: 10s
    tls:
      insecure: true
    databases:
      - postgres
    
  # PostgresSlowQueries equivalent
  sqlquery/slow_queries:
    driver: postgres
    datasource: postgres://postgres:pass@localhost:5432/postgres?sslmode=disable
    collection_interval: 15s
    queries:
      - sql: |
          WITH query_stats AS (
            SELECT 
              s.queryid::text as query_id,
              s.query as query_text,
              d.datname as database_name,
              s.calls as execution_count,
              s.mean_exec_time as avg_elapsed_time_ms,
              s.total_exec_time as total_time_ms,
              s.rows as total_rows,
              s.shared_blks_read::float / NULLIF(s.calls, 0) as avg_disk_reads,
              s.shared_blks_written::float / NULLIF(s.calls, 0) as avg_disk_writes,
              CASE 
                WHEN s.query ~* '^\s*SELECT' THEN 'SELECT'
                WHEN s.query ~* '^\s*INSERT' THEN 'INSERT'
                WHEN s.query ~* '^\s*UPDATE' THEN 'UPDATE'
                WHEN s.query ~* '^\s*DELETE' THEN 'DELETE'
                ELSE 'OTHER'
              END as statement_type,
              'public' as schema_name,
              s.mean_exec_time as avg_cpu_time_ms
            FROM pg_stat_statements s
            LEFT JOIN pg_database d ON d.oid = s.dbid
            WHERE s.mean_exec_time > 10  -- Lower threshold for testing
          )
          SELECT * FROM query_stats WHERE database_name IS NOT NULL
        metrics:
          - metric_name: postgres.slow_queries.count
            value_column: execution_count
            value_type: int
            attribute_columns: [query_id, query_text, database_name, statement_type, schema_name]
          - metric_name: postgres.slow_queries.elapsed_time
            value_column: avg_elapsed_time_ms
            value_type: double
            unit: ms
            attribute_columns: [query_id, query_text, database_name, statement_type, schema_name]
          - metric_name: postgres.slow_queries.disk_reads
            value_column: avg_disk_reads
            value_type: double
            attribute_columns: [query_id, query_text, database_name, statement_type, schema_name]
          - metric_name: postgres.slow_queries.disk_writes
            value_column: avg_disk_writes
            value_type: double
            attribute_columns: [query_id, query_text, database_name, statement_type, schema_name]
          - metric_name: postgres.slow_queries.cpu_time
            value_column: avg_cpu_time_ms
            value_type: double
            unit: ms
            attribute_columns: [query_id, query_text, database_name]
            
  # PostgresWaitEvents equivalent
  sqlquery/wait_events:
    driver: postgres
    datasource: postgres://postgres:pass@localhost:5432/postgres?sslmode=disable
    collection_interval: 10s
    queries:
      - sql: |
          SELECT 
            COALESCE(wait_event, 'None') as wait_event_name,
            COALESCE(wait_event_type, 'None') as wait_category,
            COUNT(*) * 10 as total_wait_time_ms,  -- Approximation: count * collection_interval
            COALESCE(datname, 'postgres') as database_name,
            COALESCE(LEFT(query, 100), 'idle') as query_sample,
            COALESCE(MD5(query), 'unknown') as query_id
          FROM pg_stat_activity
          WHERE state != 'idle'
          GROUP BY wait_event, wait_event_type, datname, query
        metrics:
          - metric_name: postgres.wait_events
            value_column: total_wait_time_ms
            value_type: double
            unit: ms
            attribute_columns: [wait_event_name, wait_category, database_name, query_id]
            
  # PostgresBlockingSessions equivalent
  sqlquery/blocking_sessions:
    driver: postgres
    datasource: postgres://postgres:pass@localhost:5432/postgres?sslmode=disable
    collection_interval: 10s
    queries:
      - sql: |
          SELECT
            blocked.pid::text as blocked_pid,
            blocked.query as blocked_query,
            MD5(blocked.query) as blocked_query_id,
            EXTRACT(EPOCH FROM blocked.query_start)::text as blocked_query_start,
            blocked.datname as database_name,
            blocking.pid::text as blocking_pid,
            blocking.query as blocking_query,
            MD5(blocking.query) as blocking_query_id,
            EXTRACT(EPOCH FROM blocking.query_start)::text as blocking_query_start,
            blocking.datname as blocking_database,
            1 as blocking_count
          FROM pg_stat_activity blocked
          JOIN pg_stat_activity blocking 
            ON blocking.pid = ANY(pg_blocking_pids(blocked.pid))
          WHERE blocked.wait_event IS NOT NULL
        metrics:
          - metric_name: postgres.blocking_sessions
            value_column: blocking_count
            value_type: int
            attribute_columns: [blocked_pid, blocked_query, blocked_query_id, blocked_query_start, 
                              database_name, blocking_pid, blocking_query, blocking_query_id, 
                              blocking_query_start, blocking_database]
                              
  # PostgresIndividualQueries equivalent
  sqlquery/individual_queries:
    driver: postgres
    datasource: postgres://postgres:pass@localhost:5432/postgres?sslmode=disable
    collection_interval: 30s
    queries:
      - sql: |
          SELECT 
            queryid::text as query_id,
            query as query_text,
            mean_exec_time as avg_cpu_time_ms,
            queryid::text as plan_id  -- Simplified: using query_id as plan_id
          FROM pg_stat_statements
          WHERE calls > 0
        metrics:
          - metric_name: postgres.individual_queries.cpu_time
            value_column: avg_cpu_time_ms
            value_type: double
            unit: ms
            attribute_columns: [query_id, query_text, plan_id]
            
  # PostgresExecutionPlanMetrics equivalent (simplified)
  sqlquery/execution_plans:
    driver: postgres
    datasource: postgres://postgres:pass@localhost:5432/postgres?sslmode=disable
    collection_interval: 60s
    queries:
      - sql: |
          SELECT 
            queryid::text as query_id,
            queryid::text as plan_id,
            1 as level_id,
            'SeqScan' as node_type,
            query as query_text,
            total_exec_time as total_cost,
            mean_exec_time as startup_cost,
            rows as plan_rows,
            mean_exec_time * 0.1 as actual_startup_time,
            mean_exec_time as actual_total_time,
            rows as actual_rows,
            calls as actual_loops,
            shared_blks_hit as shared_hit_block,
            shared_blks_read as shared_read_blocks,
            shared_blks_dirtied as shared_dirtied_blocks,
            shared_blks_written as shared_written_blocks,
            local_blks_hit as local_hit_block,
            local_blks_read as local_read_blocks,
            local_blks_dirtied as local_dirtied_blocks,
            local_blks_written as local_written_blocks,
            temp_blks_read as temp_read_block,
            temp_blks_written as temp_written_blocks,
            'postgres' as database_name
          FROM pg_stat_statements
          WHERE calls > 0
          LIMIT 100
        metrics:
          - metric_name: postgres.execution_plan.cost
            value_column: total_cost
            value_type: double
            attribute_columns: [query_id, plan_id, level_id, node_type, database_name]
          - metric_name: postgres.execution_plan.time
            value_column: actual_total_time
            value_type: double
            unit: ms
            attribute_columns: [query_id, plan_id, level_id, node_type, database_name]
          - metric_name: postgres.execution_plan.rows
            value_column: actual_rows
            value_type: int
            attribute_columns: [query_id, plan_id, level_id, node_type, database_name]
          - metric_name: postgres.execution_plan.blocks_hit
            value_column: shared_hit_block
            value_type: int
            attribute_columns: [query_id, plan_id, level_id, node_type, database_name]
          - metric_name: postgres.execution_plan.blocks_read
            value_column: shared_read_blocks
            value_type: int
            attribute_columns: [query_id, plan_id, level_id, node_type, database_name]

processors:
  # Add db.system attribute to all metrics
  attributes:
    actions:
      - key: db.system
        value: postgresql
        action: insert
        
  # Add resource attributes
  resource:
    attributes:
      - key: environment
        value: e2e-test
        action: upsert
      - key: service.name
        value: database-intelligence
        action: upsert
        
  # Transform metrics to match OHI event structure
  transform:
    metric_statements:
      - context: datapoint
        statements:
          # Map database_name to db.name
          - set(attributes["db.name"], attributes["database_name"]) where attributes["database_name"] != nil
          - delete_key(attributes, "database_name") where attributes["db.name"] != nil
          
          # Map query attributes
          - set(attributes["db.statement"], attributes["query_text"]) where attributes["query_text"] != nil
          - set(attributes["db.postgresql.query_id"], attributes["query_id"]) where attributes["query_id"] != nil
          - set(attributes["db.operation"], attributes["statement_type"]) where attributes["statement_type"] != nil
          - set(attributes["db.schema"], attributes["schema_name"]) where attributes["schema_name"] != nil
          
          # Map wait event attributes
          - set(attributes["db.wait_event.name"], attributes["wait_event_name"]) where attributes["wait_event_name"] != nil
          - set(attributes["db.wait_event.category"], attributes["wait_category"]) where attributes["wait_category"] != nil
          
          # Map blocking session attributes
          - set(attributes["db.blocking.blocked_pid"], attributes["blocked_pid"]) where attributes["blocked_pid"] != nil
          - set(attributes["db.blocking.blocking_pid"], attributes["blocking_pid"]) where attributes["blocking_pid"] != nil
          
          # Map execution plan attributes
          - set(attributes["db.plan.node_type"], attributes["node_type"]) where attributes["node_type"] != nil
          - set(attributes["db.plan.level"], attributes["level_id"]) where attributes["level_id"] != nil
          - set(attributes["db.postgresql.plan_id"], attributes["plan_id"]) where attributes["plan_id"] != nil
        
  # Memory limiter
  memory_limiter:
    check_interval: 1s
    limit_percentage: 75
    spike_limit_percentage: 30
    
  # Batch processor
  batch:
    timeout: 10s
    send_batch_size: 1000

exporters:
  # New Relic exporter
  otlp:
    endpoint: https://otlp.nr-data.net:4317
    headers:
      api-key: ${NEW_RELIC_LICENSE_KEY}
    compression: gzip
    
  # Debug exporter for troubleshooting
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 20

service:
  extensions: [health_check, zpages]
  
  pipelines:
    metrics:
      receivers: [postgresql, sqlquery/slow_queries, sqlquery/wait_events, 
                  sqlquery/blocking_sessions, sqlquery/individual_queries, 
                  sqlquery/execution_plans]
      processors: [attributes, resource, transform, memory_limiter, batch]
      exporters: [otlp, debug]
      
  telemetry:
    logs:
      level: info
      initial_fields:
        service: database-intelligence-collector