# Continuous Validation Configuration
# This file configures the continuous validation platform for OHI parity monitoring

schedules:
  # Quick validation runs hourly on critical widgets
  quick_validation: "0 0 * * * *"  # Every hour
  
  # Comprehensive validation runs daily at 2 AM
  comprehensive_validation: "0 0 2 * * *"  # Daily at 2 AM
  
  # Trend analysis runs weekly on Sundays at 3 AM
  trend_analysis: "0 0 3 * * 0"  # Weekly on Sunday at 3 AM
  
  # Drift detection runs every 4 hours
  drift_detection: "0 0 */4 * * *"  # Every 4 hours

thresholds:
  # Critical accuracy threshold - alerts sent when below this
  critical_accuracy: 0.90  # 90%
  
  # Warning accuracy threshold
  warning_accuracy: 0.95  # 95%
  
  # Drift detection threshold
  drift_threshold: 0.02  # 2% drift
  
  # Metric-specific thresholds
  metric_thresholds:
    "Database Query Distribution": 0.95
    "Average Execution Time": 0.95
    "Top Wait Events": 0.90
    "Execution Counts Timeline": 0.90
    "Top N Slowest Queries": 0.95
    "Disk IO Usage": 0.90
    "Blocking Details": 0.95
    "Query Details": 0.95
    "Execution Plan Metrics": 0.90

alerting:
  enabled: true
  channels:
    - webhook
    - email
    - slack
  
  webhook_url: "${VALIDATION_WEBHOOK_URL}"
  
  email_recipients:
    - "ops-team@example.com"
    - "database-team@example.com"
  
  slack_channel: "#database-monitoring"
  
  # Throttle alerts to prevent spam
  alert_throttling: 1h
  
  # Only send alerts for these severities
  severity_filters:
    - CRITICAL
    - HIGH

reporting:
  # Output directory for reports
  output_dir: "./validation-reports"
  
  # Report formats to generate
  formats:
    - json
    - yaml
    - html
  
  # How long to keep reports
  retention_days: 90
  
  # Include raw data in reports
  include_raw_data: false
  
  # Generate dashboard-ready reports
  generate_dashboards: true

drift_detection:
  enabled: true
  
  # Time window for baseline calculation
  baseline_window: 168h  # 7 days
  
  # Time window for drift detection
  detection_window: 24h  # 1 day
  
  # Minimum data points required for drift detection
  min_data_points: 10
  
  # Anomaly threshold percentage
  anomaly_threshold: 2.0  # 2%

auto_remediation:
  enabled: true
  
  # Maximum retry attempts
  max_retries: 3
  
  # Retry interval
  retry_interval: 5m
  
  # Remediation strategies
  strategies:
    - name: "high_cardinality_reduction"
      trigger: "CARDINALITY_HIGH"
      actions:
        - type: "adjust_sampling"
          config:
            sampling_reduction: 0.5
          timeout: 30s
        - type: "update_filters"
          config:
            add_filters:
              - "db.query.calls > 100"
          timeout: 30s
      max_attempts: 2
    
    - name: "missing_data_recovery"
      trigger: "MISSING_DATA"
      actions:
        - type: "restart_collector"
          config:
            graceful: true
          timeout: 2m
        - type: "check_connectivity"
          config:
            endpoints:
              - "postgresql:5432"
              - "mysql:3306"
          timeout: 30s
      max_attempts: 3
    
    - name: "value_mismatch_investigation"
      trigger: "VALUE_MISMATCH"
      actions:
        - type: "regenerate_mappings"
          config:
            force: true
          timeout: 1m
        - type: "clear_cache"
          config:
            caches:
              - "metric_cache"
              - "query_cache"
          timeout: 30s
      max_attempts: 2
    
    - name: "drift_correction"
      trigger: "DRIFT_HIGH"
      actions:
        - type: "recalibrate_baseline"
          config:
            window: "24h"
          timeout: 5m
        - type: "update_tolerances"
          config:
            increase_by: 0.01
            max_tolerance: 0.10
          timeout: 30s
      max_attempts: 1

# Validation profiles for different scenarios
validation_profiles:
  production:
    name: "Production Validation"
    description: "Strict validation for production environments"
    tolerance: 0.02  # 2%
    metrics:
      - "all"
    attributes:
      - "query_id"
      - "database_name"
      - "execution_count"
      - "avg_elapsed_time_ms"
    custom_rules:
      - name: "query_count_validation"
        type: "threshold"
        config:
          min_queries: 1000
          max_queries: 1000000
  
  staging:
    name: "Staging Validation"
    description: "Relaxed validation for staging environments"
    tolerance: 0.05  # 5%
    metrics:
      - "critical_only"
    attributes:
      - "query_id"
      - "database_name"
    custom_rules:
      - name: "basic_connectivity"
        type: "health_check"
        config:
          timeout: 30s
  
  development:
    name: "Development Validation"
    description: "Minimal validation for development"
    tolerance: 0.10  # 10%
    metrics:
      - "basic"
    attributes:
      - "database_name"
    custom_rules: []

# Additional configuration
debug:
  # Enable debug logging
  enabled: false
  
  # Log validation details
  log_validation_details: false
  
  # Save debug artifacts
  save_debug_artifacts: false
  
  # Debug output directory
  debug_output_dir: "./validation-debug"

performance:
  # Maximum concurrent validations
  max_concurrent_validations: 10
  
  # Validation timeout
  validation_timeout: 5m
  
  # Query timeout
  query_timeout: 30s
  
  # Cache settings
  cache:
    enabled: true
    ttl: 1h
    max_size: 1000

# Integration settings
integrations:
  # New Relic integration
  newrelic:
    enabled: true
    account_id: "${NEW_RELIC_ACCOUNT_ID}"
    api_key: "${NEW_RELIC_API_KEY}"
    region: "US"  # US or EU
    
  # PagerDuty integration
  pagerduty:
    enabled: false
    integration_key: "${PAGERDUTY_KEY}"
    
  # Datadog integration  
  datadog:
    enabled: false
    api_key: "${DATADOG_API_KEY}"
    app_key: "${DATADOG_APP_KEY}"