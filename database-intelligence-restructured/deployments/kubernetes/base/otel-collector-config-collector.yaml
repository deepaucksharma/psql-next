apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config-collector
  namespace: db-intelligence
data:
  collector.yaml: |
    extensions:
      health_check:
        endpoint: 0.0.0.0:13133

      memory_ballast:
        size_mib: 128

    receivers:
      sqlquery/postgresql:
        driver: postgres
        dsn: "${env:PG_REPLICA_DSN}"
        collection_interval: 300s
        timeout: 10s
        queries:
          - sql: |
              -- Safety timeouts to prevent long-running queries
              SET LOCAL statement_timeout = '3000ms';
              SET LOCAL lock_timeout = '100ms';

              -- Find the worst-performing query based on total execution time
              WITH worst_query AS (
                SELECT
                  queryid,
                  query,
                  mean_exec_time,
                  calls,
                  total_exec_time,
                  (mean_exec_time * calls) as impact_score
                FROM pg_stat_statements
                WHERE
                  mean_exec_time > 50        -- Only consider queries over 50ms
                  AND calls > 5              -- And those that are somewhat frequent
                  AND query NOT LIKE '%pg_%' -- Exclude system queries
                  AND query NOT LIKE '%EXPLAIN%'
                  AND query NOT LIKE '%SET LOCAL%'
                ORDER BY impact_score DESC
                LIMIT 1
              )
              SELECT
                queryid::text as query_id,
                query as query_text,
                round(mean_exec_time::numeric, 2) as avg_duration_ms,
                calls as execution_count,
                round(total_exec_time::numeric, 2) as total_duration_ms,
                -- This is a placeholder for the plan, as we are not collecting actual plans
                json_build_object(
                  'plan_available', false,
                  'approach', 'metadata_only_for_safety'
                ) as plan_metadata
              FROM worst_query;

      sqlquery/mysql:
        driver: mysql
        dsn: "${env:MYSQL_READONLY_DSN}"
        collection_interval: 300s
        timeout: 10s
        queries:
          - sql: |
              SELECT
                DIGEST as query_id,
                DIGEST_TEXT as query_text,
                ROUND(AVG_TIMER_WAIT/1000000, 2) as avg_duration_ms,
                COUNT_STAR as execution_count,
                ROUND((AVG_TIMER_WAIT * COUNT_STAR)/1000000, 2) as total_duration_ms,
                JSON_OBJECT(
                  'plan_available', false,
                  'approach', 'performance_schema_metadata'
                ) as plan_metadata
              FROM performance_schema.events_statements_summary_by_digest
              WHERE
                SCHEMA_NAME = DATABASE()
                AND SCHEMA_NAME IS NOT NULL
                AND AVG_TIMER_WAIT > 50000000  -- 50ms+
                AND COUNT_STAR > 5
              ORDER BY (AVG_TIMER_WAIT * COUNT_STAR) DESC
              LIMIT 1;

    processors:
      memory_limiter:
        check_interval: 2s
        limit_mib: 1024
        spike_limit_mib: 256

      transform/sanitize_pii:
        error_mode: ignore
        log_statements:
          - context: log
            statements:
              - replace_all_patterns(attributes["query_text"], "'[^']*'", "'[REDACTED]'")
              - replace_all_patterns(attributes["query_text"], "\b\d{6,}\b", "[ID]")
              - replace_all_patterns(body, "\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "[EMAIL_REDACTED]")

      resource:
        attributes:
          - key: "collector.instance.id"
            value: "${env:HOSTNAME}"
            action: upsert

      probabilistic_sampler:
        hash_seed: 22
        sampling_percentage: 25

      batch:
        timeout: 30s
        send_batch_size: 50
        send_batch_max_size: 100

exporters:
  otlp/newrelic:
    endpoint: "${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4317}"
    headers:
      api-key: "${env:NEW_RELIC_LICENSE_KEY}"
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 120s
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 256

service:
  extensions: [health_check, memory_ballast]
  pipelines:
    logs/database_intelligence:
      receivers: [sqlquery/postgresql, sqlquery/mysql]
      processors: [memory_limiter, transform/sanitize_pii, resource, probabilistic_sampler, batch]
      exporters: [otlp/newrelic]
  telemetry:
    logs:
      level: info
      encoding: json
    metrics:
      level: detailed
      address: 0.0.0.0:8888