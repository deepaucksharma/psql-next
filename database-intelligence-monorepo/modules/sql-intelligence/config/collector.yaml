# SQL Intelligence - Ultra-Comprehensive Configuration
# Pushes boundaries of OpenTelemetry + MySQL for maximum New Relic value
# Single source of truth for all SQL intelligence collection

# =====================================================
# RECEIVERS - Advanced MySQL Performance Schema Queries
# =====================================================

receivers:
  # Enhanced query analysis with comprehensive scoring
  sqlquery/enhanced_query_analysis:
    driver: mysql
    datasource: "${env:MYSQL_USER}:${env:MYSQL_PASSWORD}@tcp(${env:MYSQL_ENDPOINT})/performance_schema"
    collection_interval: 30s
    queries:
      - sql: |
          SELECT 
            DIGEST,
            SCHEMA_NAME,
            DIGEST_TEXT,
            COUNT_STAR as executions,
            AVG_TIMER_WAIT/1000000000 as avg_latency_ms,
            SUM_TIMER_WAIT/1000000000 as total_time_sec,
            MAX_TIMER_WAIT/1000000000 as max_latency_ms,
            MIN_TIMER_WAIT/1000000000 as min_latency_ms,
            SUM_ROWS_EXAMINED as rows_examined,
            SUM_ROWS_SENT as rows_sent,
            SUM_ROWS_AFFECTED as rows_affected,
            CASE 
              WHEN SUM_ROWS_SENT > 0 
              THEN SUM_ROWS_EXAMINED / SUM_ROWS_SENT 
              ELSE 0 
            END as examination_ratio,
            SUM_NO_INDEX_USED as no_index_used,
            SUM_NO_GOOD_INDEX_USED as no_good_index_used,
            CASE 
              WHEN COUNT_STAR > 0 
              THEN ((COUNT_STAR - SUM_NO_INDEX_USED) / COUNT_STAR) * 100
              ELSE 100 
            END as index_usage_pct,
            SUM_CREATED_TMP_DISK_TABLES as tmp_disk_tables,
            SUM_CREATED_TMP_TABLES as tmp_tables,
            CASE 
              WHEN COUNT_STAR > 0 
              THEN (SUM_CREATED_TMP_DISK_TABLES / COUNT_STAR) * 100
              ELSE 0 
            END as disk_tmp_table_pct,
            SUM_SORT_MERGE_PASSES as sort_merge_passes,
            SUM_SORT_ROWS as sort_rows,
            SUM_SELECT_FULL_JOIN as full_joins,
            SUM_SELECT_SCAN as full_table_scans,
            SUM_SORT_SCAN as sort_scans,
            -- Enhanced cost score calculation (0-100, higher is worse)
            LEAST(100, 
              -- Latency factor (0-35 points)
              (AVG_TIMER_WAIT/1000000000 / 10) * 0.35 +  
              -- Efficiency factor (0-30 points)  
              (CASE 
                WHEN SUM_ROWS_SENT > 0 
                THEN LOG10(GREATEST(SUM_ROWS_EXAMINED / SUM_ROWS_SENT, 1)) * 3
                ELSE 0 
              END) * 0.30 +  
              -- Index usage factor (0-20 points)
              (SUM_NO_INDEX_USED / GREATEST(COUNT_STAR, 1) * 100) * 0.20 +
              -- Temporary table factor (0-10 points)
              (CASE 
                WHEN COUNT_STAR > 0 
                THEN (SUM_CREATED_TMP_DISK_TABLES / COUNT_STAR) * 100 * 0.10
                ELSE 0 
              END) +
              -- Full scan penalty (0-5 points)
              (SUM_SELECT_SCAN / GREATEST(COUNT_STAR, 1) * 100) * 0.05
            ) as query_cost_score,
            -- Performance tier classification
            CASE
              WHEN (AVG_TIMER_WAIT/1000000000) > 1000 THEN 'critical'
              WHEN (AVG_TIMER_WAIT/1000000000) > 100 THEN 'slow'
              WHEN (AVG_TIMER_WAIT/1000000000) > 10 THEN 'moderate'
              ELSE 'fast'
            END as performance_tier,
            -- Query pattern classification
            CASE
              WHEN DIGEST_TEXT LIKE 'SELECT%' AND SUM_ROWS_AFFECTED = 0 THEN 'read_heavy'
              WHEN DIGEST_TEXT LIKE 'INSERT%' OR DIGEST_TEXT LIKE 'UPDATE%' OR DIGEST_TEXT LIKE 'DELETE%' THEN 'write_heavy'
              WHEN DIGEST_TEXT LIKE 'CREATE%' OR DIGEST_TEXT LIKE 'DROP%' OR DIGEST_TEXT LIKE 'ALTER%' THEN 'ddl'
              ELSE 'mixed'
            END as query_pattern,
            -- Optimization priority scoring
            CASE
              WHEN COUNT_STAR > 100 AND (AVG_TIMER_WAIT/1000000000) > 100 THEN 'critical'
              WHEN COUNT_STAR > 50 AND (SUM_NO_INDEX_USED / COUNT_STAR) > 0.5 THEN 'high'
              WHEN (SUM_ROWS_EXAMINED / GREATEST(SUM_ROWS_SENT, 1)) > 10 THEN 'medium'
              ELSE 'low'
            END as optimization_priority,
            -- Enhanced business impact calculation
            LEAST(100,
              -- Frequency weight (0-40 points)
              (COUNT_STAR / 60) * 0.40 +  
              -- Latency weight (0-30 points)
              (AVG_TIMER_WAIT/1000000000 / 100) * 0.30 +  
              -- Resource consumption weight (0-20 points)
              ((SUM_ROWS_EXAMINED + SUM_SORT_ROWS) / 10000) * 0.20 +
              -- Inefficiency penalty (0-10 points)
              (SUM_NO_INDEX_USED / GREATEST(COUNT_STAR, 1) * 100) * 0.10
            ) as business_impact_score
          FROM performance_schema.events_statements_summary_by_digest
          WHERE SCHEMA_NAME IS NOT NULL
            AND DIGEST_TEXT NOT LIKE '%performance_schema%'
            AND DIGEST_TEXT NOT LIKE '%information_schema%'
            AND COUNT_STAR > 0  -- Only executed queries
          ORDER BY business_impact_score DESC, query_cost_score DESC
          LIMIT 150
        
        metrics:
          - metric_name: mysql.query.intelligence.enhanced
            value_column: "query_cost_score"
            data_type: gauge
            value_type: double
            attribute_columns: [
              DIGEST, DIGEST_TEXT, SCHEMA_NAME, executions,
              avg_latency_ms, max_latency_ms, min_latency_ms, total_time_sec, 
              rows_examined, rows_sent, rows_affected, examination_ratio, 
              no_index_used, no_good_index_used, index_usage_pct,
              tmp_disk_tables, tmp_tables, disk_tmp_table_pct,
              sort_merge_passes, sort_rows, full_joins, full_table_scans, sort_scans,
              performance_tier, query_pattern, optimization_priority, business_impact_score
            ]

  # Query execution plan analysis for deep insights  
  sqlquery/execution_plan_analysis:
    driver: mysql
    datasource: "${env:MYSQL_USER}:${env:MYSQL_PASSWORD}@tcp(${env:MYSQL_ENDPOINT})/performance_schema"
    collection_interval: 60s
    queries:
      - sql: |
          SELECT 
            sq.DIGEST,
            sq.SCHEMA_NAME,
            sq.DIGEST_TEXT,
            sq.COUNT_STAR as executions,
            sq.AVG_TIMER_WAIT/1000000000 as avg_latency_ms,
            -- Join with events_stages for execution stage analysis
            COALESCE(st.stage_analysis, 'no_stage_data') as execution_stages,
            -- Calculate execution complexity score
            CASE
              WHEN sq.DIGEST_TEXT LIKE '%JOIN%' THEN 'complex_join'
              WHEN sq.DIGEST_TEXT LIKE '%SUBQUERY%' OR sq.DIGEST_TEXT LIKE '%(%SELECT%' THEN 'subquery'
              WHEN sq.DIGEST_TEXT LIKE '%ORDER BY%' AND sq.DIGEST_TEXT LIKE '%GROUP BY%' THEN 'complex_sort'
              WHEN sq.DIGEST_TEXT LIKE '%UNION%' THEN 'union_query'
              WHEN sq.DIGEST_TEXT LIKE '%DISTINCT%' THEN 'distinct_query'
              ELSE 'simple'
            END as query_complexity,
            -- Estimate execution cost based on operations
            LEAST(100,
              -- Base complexity score
              CASE
                WHEN sq.DIGEST_TEXT LIKE '%JOIN%' THEN 30
                WHEN sq.DIGEST_TEXT LIKE '%SUBQUERY%' THEN 25  
                WHEN sq.DIGEST_TEXT LIKE '%ORDER BY%' THEN 15
                WHEN sq.DIGEST_TEXT LIKE '%GROUP BY%' THEN 20
                WHEN sq.DIGEST_TEXT LIKE '%DISTINCT%' THEN 10
                ELSE 5
              END +
              -- Latency penalty
              (sq.AVG_TIMER_WAIT/1000000000 / 10) * 0.3 +
              -- Frequency amplifier  
              (sq.COUNT_STAR / 100) * 0.2
            ) as execution_complexity_score,
            -- Query optimization suggestions
            CASE
              WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.8 THEN 'add_missing_indexes'
              WHEN sq.DIGEST_TEXT LIKE '%ORDER BY%' AND sq.SUM_SORT_ROWS > sq.COUNT_STAR * 1000 THEN 'optimize_sorting'
              WHEN sq.DIGEST_TEXT LIKE '%JOIN%' AND sq.AVG_TIMER_WAIT/1000000000 > 100 THEN 'optimize_joins'
              WHEN sq.SUM_CREATED_TMP_DISK_TABLES > 0 THEN 'increase_tmp_table_size'
              WHEN sq.SUM_SELECT_FULL_JOIN > 0 THEN 'fix_cartesian_products'
              ELSE 'well_optimized'
            END as optimization_suggestion,
            -- Performance impact assessment
            CASE
              WHEN sq.COUNT_STAR > 1000 AND sq.AVG_TIMER_WAIT/1000000000 > 500 THEN 'critical_impact'
              WHEN sq.COUNT_STAR > 100 AND sq.AVG_TIMER_WAIT/1000000000 > 100 THEN 'high_impact'
              WHEN sq.COUNT_STAR > 10 AND sq.AVG_TIMER_WAIT/1000000000 > 50 THEN 'medium_impact'
              ELSE 'low_impact'
            END as performance_impact
          FROM performance_schema.events_statements_summary_by_digest sq
          LEFT JOIN (
            SELECT 
              OBJECT_SCHEMA,
              GROUP_CONCAT(DISTINCT EVENT_NAME ORDER BY EVENT_NAME) as stage_analysis
            FROM performance_schema.events_stages_summary_by_stage_by_digest
            WHERE OBJECT_SCHEMA IS NOT NULL
            GROUP BY OBJECT_SCHEMA
            LIMIT 50
          ) st ON sq.SCHEMA_NAME = st.OBJECT_SCHEMA
          WHERE sq.SCHEMA_NAME IS NOT NULL
            AND sq.DIGEST_TEXT NOT LIKE '%performance_schema%'
            AND sq.DIGEST_TEXT NOT LIKE '%information_schema%'
            AND sq.COUNT_STAR > 0
            AND sq.AVG_TIMER_WAIT/1000000000 > 1  -- Focus on queries taking >1ms
          ORDER BY execution_complexity_score DESC, sq.COUNT_STAR DESC
          LIMIT 100
          
        metrics:
          - metric_name: mysql.query.execution.complexity
            value_column: "execution_complexity_score"
            data_type: gauge
            value_type: double
            attribute_columns: [
              DIGEST, SCHEMA_NAME, DIGEST_TEXT, executions, avg_latency_ms,
              query_complexity, optimization_suggestion, performance_impact, execution_stages
            ]

  # Query recommendation engine with specific suggestions
  sqlquery/query_recommendations:
    driver: mysql
    datasource: "${env:MYSQL_USER}:${env:MYSQL_PASSWORD}@tcp(${env:MYSQL_ENDPOINT})/performance_schema"
    collection_interval: 120s
    queries:
      - sql: |
          SELECT 
            sq.DIGEST,
            sq.SCHEMA_NAME,
            sq.DIGEST_TEXT,
            sq.COUNT_STAR as executions,
            sq.AVG_TIMER_WAIT/1000000000 as avg_latency_ms,
            sq.SUM_TIMER_WAIT/1000000000 as total_time_sec,
            -- Priority-based recommendations
            CASE
              WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.9 THEN 'critical'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 1000 AND sq.COUNT_STAR > 10 THEN 'critical'
              WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.5 THEN 'high'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 100 AND sq.COUNT_STAR > 5 THEN 'high'
              WHEN sq.SUM_CREATED_TMP_DISK_TABLES > 0 THEN 'medium'
              WHEN sq.SUM_SELECT_FULL_JOIN > 0 THEN 'medium'
              ELSE 'low'
            END as recommendation_priority,
            -- Specific actionable recommendations
            CONCAT_WS('; ',
              CASE 
                WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.8 
                THEN CONCAT('ADD INDEX: Create indexes for frequently scanned tables (', 
                     ROUND((sq.SUM_NO_INDEX_USED / sq.COUNT_STAR) * 100, 1), '% scans without index)')
                ELSE NULL 
              END,
              CASE 
                WHEN sq.SUM_CREATED_TMP_DISK_TABLES > 0 
                THEN CONCAT('MEMORY CONFIG: Increase tmp_table_size (', sq.SUM_CREATED_TMP_DISK_TABLES, ' disk tmp tables)')
                ELSE NULL 
              END,
              CASE 
                WHEN sq.SUM_SELECT_FULL_JOIN > 0 
                THEN 'JOIN OPTIMIZATION: Review JOIN conditions for cartesian products'
                ELSE NULL 
              END,
              CASE 
                WHEN sq.AVG_TIMER_WAIT/1000000000 > 1000 
                THEN CONCAT('QUERY REWRITE: Average latency ', ROUND(sq.AVG_TIMER_WAIT/1000000000, 2), 'ms exceeds threshold')
                ELSE NULL 
              END,
              CASE 
                WHEN sq.SUM_SORT_ROWS > sq.COUNT_STAR * 10000 
                THEN CONCAT('SORT OPTIMIZATION: Large sort operations (', sq.SUM_SORT_ROWS, ' rows sorted)')
                ELSE NULL 
              END
            ) as specific_recommendations,
            -- Estimated performance improvement
            CASE
              WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.8 THEN '50-80%'
              WHEN sq.SUM_CREATED_TMP_DISK_TABLES > sq.COUNT_STAR * 0.5 THEN '30-50%'
              WHEN sq.SUM_SELECT_FULL_JOIN > 0 THEN '60-90%'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 1000 THEN '40-70%'
              ELSE '10-30%'
            END as estimated_improvement,
            -- Implementation complexity
            CASE
              WHEN sq.DIGEST_TEXT LIKE '%JOIN%' AND sq.SUM_SELECT_FULL_JOIN > 0 THEN 'complex'
              WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.8 THEN 'medium'
              WHEN sq.SUM_CREATED_TMP_DISK_TABLES > 0 THEN 'easy'
              ELSE 'easy'
            END as implementation_complexity,
            -- Business impact if optimized
            LEAST(100,
              (sq.COUNT_STAR * sq.AVG_TIMER_WAIT/1000000000 / 1000) * 0.6 +  -- Time savings
              (sq.COUNT_STAR / 100) * 0.4  -- Frequency impact
            ) as optimization_business_impact,
            -- Resource savings potential
            CASE
              WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.8 THEN 'cpu_intensive'
              WHEN sq.SUM_CREATED_TMP_DISK_TABLES > 0 THEN 'disk_io_intensive'
              WHEN sq.SUM_SORT_ROWS > sq.COUNT_STAR * 1000 THEN 'memory_intensive'
              ELSE 'minimal_resource_impact'
            END as resource_savings_type
          FROM performance_schema.events_statements_summary_by_digest sq
          WHERE sq.SCHEMA_NAME IS NOT NULL
            AND sq.DIGEST_TEXT NOT LIKE '%performance_schema%'
            AND sq.DIGEST_TEXT NOT LIKE '%information_schema%'
            AND sq.COUNT_STAR > 0
            AND (
              sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.3  -- Has index issues
              OR sq.AVG_TIMER_WAIT/1000000000 > 50  -- Slow queries
              OR sq.SUM_CREATED_TMP_DISK_TABLES > 0  -- Memory issues
              OR sq.SUM_SELECT_FULL_JOIN > 0  -- Join issues
            )
          ORDER BY optimization_business_impact DESC, sq.COUNT_STAR DESC
          LIMIT 50
          
        metrics:
          - metric_name: mysql.query.optimization.recommendations
            value_column: "optimization_business_impact"
            data_type: gauge
            value_type: double
            attribute_columns: [
              DIGEST, SCHEMA_NAME, DIGEST_TEXT, executions, avg_latency_ms, total_time_sec,
              recommendation_priority, specific_recommendations, estimated_improvement,
              implementation_complexity, resource_savings_type
            ]

  # Slow query log analysis for critical performance issues
  sqlquery/slow_query_analysis:
    driver: mysql
    datasource: "${env:MYSQL_USER}:${env:MYSQL_PASSWORD}@tcp(${env:MYSQL_ENDPOINT})/performance_schema"
    collection_interval: 300s  # Run every 5 minutes for slow query focus
    queries:
      - sql: |
          SELECT 
            sq.DIGEST,
            sq.SCHEMA_NAME,
            sq.DIGEST_TEXT,
            sq.COUNT_STAR as executions,
            sq.AVG_TIMER_WAIT/1000000000 as avg_latency_ms,
            sq.MAX_TIMER_WAIT/1000000000 as max_latency_ms,
            sq.SUM_TIMER_WAIT/1000000000 as total_time_sec,
            -- Percentile analysis for latency distribution
            CASE
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 10000 THEN 'p99_extreme'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 5000 THEN 'p95_critical'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 1000 THEN 'p90_slow'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 100 THEN 'p75_moderate'
              ELSE 'p50_acceptable'
            END as latency_percentile,
            -- Resource consumption analysis
            (sq.SUM_ROWS_EXAMINED + sq.SUM_SORT_ROWS) as total_rows_processed,
            sq.SUM_CREATED_TMP_DISK_TABLES as disk_tmp_tables,
            sq.SUM_CREATED_TMP_TABLES as memory_tmp_tables,
            -- Calculate total resource cost
            (sq.SUM_TIMER_WAIT/1000000000) * sq.COUNT_STAR as total_cpu_seconds,
            -- Severity classification
            CASE
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 5000 AND sq.COUNT_STAR > 5 THEN 'critical'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 1000 AND sq.COUNT_STAR > 10 THEN 'high'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 500 AND sq.COUNT_STAR > 20 THEN 'medium'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 100 AND sq.COUNT_STAR > 50 THEN 'low'
              ELSE 'minimal'
            END as severity_level,
            -- Root cause analysis
            CASE
              WHEN sq.SUM_NO_INDEX_USED > sq.COUNT_STAR * 0.8 THEN 'missing_indexes'
              WHEN sq.SUM_CREATED_TMP_DISK_TABLES > sq.COUNT_STAR * 0.3 THEN 'insufficient_memory'
              WHEN sq.SUM_SELECT_FULL_JOIN > 0 THEN 'cartesian_joins'
              WHEN sq.SUM_SORT_ROWS > sq.COUNT_STAR * 50000 THEN 'large_sorts'
              WHEN sq.SUM_LOCK_TIME/1000000000 > sq.SUM_TIMER_WAIT/1000000000 * 0.1 THEN 'lock_contention'
              ELSE 'query_complexity'
            END as primary_bottleneck,
            -- Performance impact on system
            LEAST(100,
              -- CPU time impact (40% weight)
              ((sq.SUM_TIMER_WAIT/1000000000) * sq.COUNT_STAR / 3600) * 0.40 +
              -- Frequency impact (30% weight)
              (sq.COUNT_STAR / 1000) * 0.30 +
              -- Resource consumption impact (30% weight)
              ((sq.SUM_ROWS_EXAMINED + sq.SUM_SORT_ROWS) / 1000000) * 0.30
            ) as system_impact_score,
            -- Time window analysis
            CASE
              WHEN HOUR(sq.LAST_SEEN) BETWEEN 9 AND 17 THEN 'business_hours'
              WHEN HOUR(sq.LAST_SEEN) BETWEEN 18 AND 23 THEN 'evening_peak'
              WHEN HOUR(sq.LAST_SEEN) BETWEEN 0 AND 6 THEN 'maintenance_window'
              ELSE 'off_hours'
            END as execution_time_pattern,
            -- Urgency for optimization
            CASE
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 10000 THEN 'immediate'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 5000 AND sq.COUNT_STAR > 5 THEN 'urgent'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 1000 AND sq.COUNT_STAR > 10 THEN 'high_priority'
              WHEN sq.AVG_TIMER_WAIT/1000000000 > 500 AND sq.COUNT_STAR > 20 THEN 'medium_priority'
              ELSE 'low_priority'
            END as optimization_urgency
          FROM performance_schema.events_statements_summary_by_digest sq
          WHERE sq.SCHEMA_NAME IS NOT NULL
            AND sq.DIGEST_TEXT NOT LIKE '%performance_schema%'
            AND sq.DIGEST_TEXT NOT LIKE '%information_schema%'
            AND sq.COUNT_STAR > 0
            AND sq.AVG_TIMER_WAIT/1000000000 > 100  -- Focus on queries >100ms
          ORDER BY system_impact_score DESC, sq.AVG_TIMER_WAIT DESC
          LIMIT 75
          
        metrics:
          - metric_name: mysql.query.slow_analysis.impact
            value_column: "system_impact_score"
            data_type: gauge
            value_type: double
            attribute_columns: [
              DIGEST, SCHEMA_NAME, DIGEST_TEXT, executions, 
              avg_latency_ms, max_latency_ms, total_time_sec, latency_percentile,
              total_rows_processed, disk_tmp_tables, memory_tmp_tables, total_cpu_seconds,
              severity_level, primary_bottleneck, execution_time_pattern, optimization_urgency
            ]

  # Table access pattern intelligence
  sqlquery/access_patterns:
    driver: mysql
    datasource: "${env:MYSQL_USER}:${env:MYSQL_PASSWORD}@tcp(${env:MYSQL_ENDPOINT})/performance_schema"
    collection_interval: 60s
    queries:
      - sql: |
          WITH time_windows AS (
            SELECT 
              OBJECT_SCHEMA,
              OBJECT_NAME,
              COUNT_READ,
              COUNT_WRITE,
              SUM_TIMER_READ/1000000000 as read_latency_sec,
              SUM_TIMER_WRITE/1000000000 as write_latency_sec,
              CURRENT_TIMESTAMP as snapshot_time,
              HOUR(CURRENT_TIMESTAMP) as hour_of_day,
              DAYOFWEEK(CURRENT_TIMESTAMP) as day_of_week
            FROM performance_schema.table_io_waits_summary_by_table
            WHERE OBJECT_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema', 'sys')
          )
          SELECT 
            OBJECT_SCHEMA,
            OBJECT_NAME,
            COUNT_READ as current_reads,
            COUNT_WRITE as current_writes,
            read_latency_sec,
            write_latency_sec,
            hour_of_day,
            day_of_week,
            -- Access pattern classification
            CASE 
              WHEN hour_of_day BETWEEN 9 AND 17 THEN 'business_hours'
              WHEN hour_of_day BETWEEN 0 AND 6 THEN 'maintenance_window'
              ELSE 'off_hours'
            END as time_category,
            -- Workload type
            CASE
              WHEN COUNT_WRITE > COUNT_READ * 3 THEN 'write_intensive'
              WHEN COUNT_READ > COUNT_WRITE * 3 THEN 'read_intensive'
              WHEN COUNT_READ + COUNT_WRITE < 100 THEN 'low_activity'
              ELSE 'mixed_workload'
            END as workload_type,
            -- Calculate IOPS
            (COUNT_READ + COUNT_WRITE) / 60 as iops_estimate
          FROM time_windows
          WHERE COUNT_READ + COUNT_WRITE > 0
        
        metrics:
          - metric_name: mysql.table.access_patterns
            value_column: "iops_estimate"
            data_type: gauge
            value_type: double
            attribute_columns: [
              OBJECT_SCHEMA, OBJECT_NAME, current_reads, current_writes,
              read_latency_sec, write_latency_sec, time_category, workload_type
            ]

  # Index effectiveness analysis
  sqlquery/index_effectiveness:
    driver: mysql
    datasource: "${env:MYSQL_USER}:${env:MYSQL_PASSWORD}@tcp(${env:MYSQL_ENDPOINT})/information_schema"
    collection_interval: 300s
    queries:
      - sql: |
          WITH index_stats AS (
            SELECT 
              s.TABLE_SCHEMA,
              s.TABLE_NAME,
              s.INDEX_NAME,
              s.COLUMN_NAME,
              s.CARDINALITY,
              s.SEQ_IN_INDEX,
              s.NULLABLE,
              s.INDEX_TYPE,
              t.TABLE_ROWS,
              t.DATA_LENGTH,
              t.INDEX_LENGTH,
              -- Index selectivity
              CASE 
                WHEN t.TABLE_ROWS > 0 
                THEN (s.CARDINALITY / t.TABLE_ROWS) * 100
                ELSE 0 
              END as selectivity_pct,
              -- Index size efficiency
              CASE 
                WHEN t.DATA_LENGTH > 0
                THEN (t.INDEX_LENGTH / t.DATA_LENGTH) * 100
                ELSE 0
              END as index_size_ratio_pct
            FROM information_schema.STATISTICS s
            JOIN information_schema.TABLES t 
              ON s.TABLE_SCHEMA = t.TABLE_SCHEMA 
              AND s.TABLE_NAME = t.TABLE_NAME
            WHERE s.TABLE_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema', 'sys')
              AND s.SEQ_IN_INDEX = 1  -- First column only for main stats
          ),
          usage_stats AS (
            SELECT 
              OBJECT_SCHEMA,
              OBJECT_NAME,
              INDEX_NAME,
              COUNT_STAR as usage_count,
              COUNT_READ as read_count,
              COUNT_WRITE as write_count
            FROM performance_schema.table_io_waits_summary_by_index_usage
            WHERE OBJECT_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema', 'sys')
              AND INDEX_NAME IS NOT NULL
          )
          SELECT 
            i.TABLE_SCHEMA,
            i.TABLE_NAME,
            i.INDEX_NAME,
            i.COLUMN_NAME,
            i.CARDINALITY,
            i.TABLE_ROWS,
            i.selectivity_pct,
            i.index_size_ratio_pct,
            i.INDEX_TYPE,
            COALESCE(u.usage_count, 0) as usage_count,
            COALESCE(u.read_count, 0) as read_count,
            COALESCE(u.write_count, 0) as write_count,
            -- Index effectiveness score
            CASE
              WHEN COALESCE(u.usage_count, 0) = 0 THEN 0  -- Unused index
              WHEN i.selectivity_pct > 95 THEN 100  -- Highly selective
              WHEN i.selectivity_pct > 80 THEN 80   -- Good selectivity
              WHEN i.selectivity_pct > 50 THEN 60   -- Moderate selectivity
              ELSE 40  -- Poor selectivity
            END as effectiveness_score,
            -- Recommendations
            CASE
              WHEN COALESCE(u.usage_count, 0) = 0 AND i.INDEX_NAME != 'PRIMARY' 
                THEN 'CONSIDER_DROPPING: Index never used'
              WHEN i.selectivity_pct < 30 AND i.INDEX_NAME != 'PRIMARY'
                THEN 'LOW_SELECTIVITY: Index may not be effective'
              WHEN i.index_size_ratio_pct > 50
                THEN 'LARGE_INDEX: Consider index size optimization'
              ELSE 'OK'
            END as index_recommendation
          FROM index_stats i
          LEFT JOIN usage_stats u 
            ON i.TABLE_SCHEMA = u.OBJECT_SCHEMA 
            AND i.TABLE_NAME = u.OBJECT_NAME 
            AND i.INDEX_NAME = u.INDEX_NAME
          ORDER BY effectiveness_score ASC, usage_count DESC
        
        metrics:
          - metric_name: mysql.index.effectiveness
            value_column: "effectiveness_score"
            data_type: gauge
            value_type: double
            attribute_columns: [
              TABLE_SCHEMA, TABLE_NAME, INDEX_NAME, COLUMN_NAME,
              selectivity_pct, usage_count, index_recommendation
            ]

  # Lock wait analysis
  sqlquery/lock_analysis:
    driver: mysql
    datasource: "${env:MYSQL_USER}:${env:MYSQL_PASSWORD}@tcp(${env:MYSQL_ENDPOINT})/performance_schema"
    collection_interval: 15s
    queries:
      - sql: |
          SELECT 
            OBJECT_SCHEMA,
            OBJECT_NAME,
            COUNT_READ as read_locks,
            COUNT_WRITE as write_locks,
            SUM_TIMER_READ/1000000000 as read_lock_wait_sec,
            SUM_TIMER_WRITE/1000000000 as write_lock_wait_sec,
            -- Lock contention score
            CASE
              WHEN COUNT_READ + COUNT_WRITE > 0
              THEN (SUM_TIMER_READ + SUM_TIMER_WRITE) / (COUNT_READ + COUNT_WRITE) / 1000000
              ELSE 0
            END as avg_lock_wait_ms,
            -- Contention level
            CASE
              WHEN (SUM_TIMER_READ + SUM_TIMER_WRITE) / 1000000000 > 10 THEN 'critical'
              WHEN (SUM_TIMER_READ + SUM_TIMER_WRITE) / 1000000000 > 5 THEN 'high'
              WHEN (SUM_TIMER_READ + SUM_TIMER_WRITE) / 1000000000 > 1 THEN 'medium'
              ELSE 'low'
            END as contention_level
          FROM performance_schema.table_lock_waits_summary_by_table
          WHERE OBJECT_SCHEMA NOT IN ('mysql', 'performance_schema', 'information_schema', 'sys')
            AND (COUNT_READ > 0 OR COUNT_WRITE > 0)
            AND (SUM_TIMER_READ > 0 OR SUM_TIMER_WRITE > 0)
          ORDER BY (SUM_TIMER_READ + SUM_TIMER_WRITE) DESC
        
        metrics:
          - metric_name: mysql.lock.contention
            value_column: "avg_lock_wait_ms"
            data_type: gauge
            value_type: double
            attribute_columns: [
              OBJECT_SCHEMA, OBJECT_NAME, read_locks, write_locks,
              read_lock_wait_sec, write_lock_wait_sec, contention_level
            ]

  # Core MySQL metrics (standard)
  mysql:
    endpoint: ${env:MYSQL_ENDPOINT}
    username: ${env:MYSQL_USER}
    password: ${env:MYSQL_PASSWORD}
    collection_interval: 10s
    initial_delay: 1s

  # Prometheus scraping for federated metrics (simplified)
  prometheus:
    config:
      scrape_configs:
        - job_name: 'sql-intelligence-self'
          scrape_interval: 30s
          static_configs:
            - targets: ['localhost:8082']

  # OTLP receiver for external data
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

# =====================================================
# PROCESSORS - Advanced Intelligence Pipeline
# =====================================================

processors:
  # Stage 1: Resource protection (always first)
  memory_limiter:
    check_interval: 5s
    limit_percentage: 80
    spike_limit_percentage: 30

  # Stage 2: Batching for efficiency
  batch:
    timeout: 5s
    send_batch_size: 1000
    send_batch_max_size: 2000

  # Stage 3: Context enrichment
  attributes:
    actions:
      - key: service.name
        value: "sql-intelligence"
        action: insert
      - key: service.version
        value: "2.0.0"
        action: insert
      - key: environment
        value: ${env:ENVIRONMENT}
        action: insert
      - key: cluster.name
        value: ${env:CLUSTER_NAME}
        action: insert

  resource:
    attributes:
      - key: mysql.endpoint
        value: ${env:MYSQL_ENDPOINT}
        action: insert
      - key: mysql.version
        from_attribute: mysql.version
        action: insert

  # Stage 4: Query intelligence enrichment
  transform/query_intelligence:
    error_mode: ignore
    metric_statements:
      - context: datapoint
        statements:
          - set(attributes["analysis_version"], "2.0")
          - set(attributes["intelligence_enabled"], "true")
  
  # Stage 5: Risk assessment enrichment
  transform/risk_assessment:
    error_mode: ignore
    metric_statements:
      - context: datapoint
        statements:
          - set(attributes["risk_assessment"], "enabled")
          - set(attributes["monitoring_tier"], "production")

  # Stage 6: Metric standardization
  metricstransform/standardize:
    transforms:
      # Rename to follow pattern: mysql.<object>.<measurement>.<unit>
      - include: mysql.query.intelligence.enhanced
        new_name: mysql.query.cost.score
        action: update
      
      - include: mysql.query.execution.complexity
        new_name: mysql.query.execution.complexity_score
        action: update
      
      - include: mysql.query.optimization.recommendations
        new_name: mysql.query.optimization.business_impact
        action: update
      
      - include: mysql.query.slow_analysis.impact
        new_name: mysql.query.slow.system_impact_score
        action: update
      
      - include: mysql.table.access_patterns
        new_name: mysql.table.iops.estimate
        action: update
      
      - include: mysql.index.effectiveness
        new_name: mysql.index.effectiveness.score
        action: update
      
      - include: mysql.lock.contention
        new_name: mysql.lock.wait.milliseconds
        action: update

  # Stage 7: New Relic integration
  attributes/newrelic:
    actions:
      - key: instrumentation.provider
        value: "opentelemetry"
        action: insert
      - key: collector.module
        value: "sql-intelligence"
        action: insert
      - key: newrelic.source
        value: "mysql.performance_schema"
        action: insert

  # Stage 8: Entity synthesis
  attributes/entity_synthesis:
    actions:
      - key: entity.type
        value: "MYSQL_QUERY_INTELLIGENCE"
        action: insert
      
      # Extract host and port dynamically
      - key: db.system
        value: "mysql"
        action: insert
      
      - key: entity.guid
        value: MYSQL_QUERY_INTEL|${env:CLUSTER_NAME}|${env:MYSQL_ENDPOINT}
        action: insert
      
      - key: entity.name
        value: "${env:CLUSTER_NAME}-sql-intelligence"
        action: insert

  # Stage 9: Priority filtering (simplified for compatibility)
  filter/critical:
    error_mode: ignore
    metrics:
      include:
        match_type: regexp
        metric_names:
          - "mysql\\.query\\..*"
          - "mysql\\.lock\\..*"

# =====================================================
# EXPORTERS - Multi-tier export strategy
# =====================================================

exporters:
  # Primary New Relic exporter
  otlphttp/newrelic:
    endpoint: ${env:NEW_RELIC_OTLP_ENDPOINT}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 60s
      max_elapsed_time: 300s
    sending_queue:
      enabled: true
      num_consumers: 5
      queue_size: 1000

  # Priority lane for critical metrics
  otlphttp/newrelic_priority:
    endpoint: ${env:NEW_RELIC_OTLP_ENDPOINT}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
      X-Priority: "high"
    compression: none  # Speed over size
    timeout: 10s
    retry_on_failure:
      enabled: true
      initial_interval: 1s
      max_interval: 10s
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000

  # Alert file for critical issues
  file/alerts:
    path: ./critical-queries.json
    rotation:
      max_megabytes: 10
      max_days: 3
      max_backups: 5
    format: json

  # Prometheus for federation
  prometheus:
    endpoint: "0.0.0.0:8082"
    namespace: mysql
    send_timestamps: true
    enable_open_metrics: true
    resource_to_telemetry_conversion:
      enabled: true

  # Debug for development
  debug:
    verbosity: detailed
    sampling_initial: 10
    sampling_thereafter: 100

# =====================================================
# EXTENSIONS - Monitoring and profiling
# =====================================================

extensions:
  # WARNING: Health check endpoint intentionally REMOVED
  # This is a deliberate security decision
  # DO NOT add health_check extension
  # Use Prometheus metrics endpoint (port 8082) for monitoring

  # Performance profiling
  pprof:
    endpoint: 0.0.0.0:1777

# =====================================================
# SERVICE - Pipeline configuration
# =====================================================

service:
  extensions: [pprof]
  
  pipelines:
    # Single unified pipeline (no duplication)
    metrics:
      receivers: [
        mysql,
        sqlquery/enhanced_query_analysis,
        sqlquery/execution_plan_analysis,
        sqlquery/query_recommendations,
        sqlquery/slow_query_analysis,
        sqlquery/access_patterns,
        sqlquery/index_effectiveness,
        sqlquery/lock_analysis,
        prometheus,
        otlp
      ]
      processors: [
        memory_limiter,
        batch,
        attributes,
        resource,
        transform/query_intelligence,
        transform/risk_assessment,
        metricstransform/standardize,
        attributes/newrelic,
        attributes/entity_synthesis
      ]
      exporters: [otlphttp/newrelic, prometheus, debug]
  
  telemetry:
    logs:
      level: info
      output_paths: [stdout]