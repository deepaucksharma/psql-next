# Plan Intelligence Overlay Configuration
# Advanced SQL plan analysis and performance regression detection

# Plan intelligence receivers
receivers:
  # ASH (Active Session History) receiver for Oracle-style monitoring
  ash:
    endpoint: ${env:POSTGRES_HOST:-localhost}:${env:POSTGRES_PORT:-5432}
    username: ${env:POSTGRES_USER:-postgres}
    password: ${env:POSTGRES_PASSWORD:-postgres}
    database: ${env:POSTGRES_DB:-postgres}
    collection_interval: ${env:ASH_COLLECTION_INTERVAL:-1s}
    session_sampling_rate: ${env:ASH_SAMPLING_RATE:-1.0}
    enable_wait_events: ${env:ENABLE_WAIT_EVENTS:-true}

  # Enhanced SQL receiver with advanced plan collection
  enhancedsql/plan-intelligence:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:-localhost} port=${env:POSTGRES_PORT:-5432} user=${env:POSTGRES_USER:-postgres} password=${env:POSTGRES_PASSWORD:-postgres} dbname=${env:POSTGRES_DB:-postgres} sslmode=disable"
    collection_interval: ${env:PLAN_INTELLIGENCE_INTERVAL:-30s}
    enable_plan_collection: true
    plan_cache_size: ${env:PLAN_CACHE_SIZE:-5000}
    auto_explain:
      enabled: ${env:AUTO_EXPLAIN_ENABLED:-true}
      log_min_duration: ${env:AUTO_EXPLAIN_MIN_DURATION:-100ms}
      log_analyze: ${env:AUTO_EXPLAIN_LOG_ANALYZE:-true}
      log_buffers: ${env:AUTO_EXPLAIN_LOG_BUFFERS:-true}
      log_timing: ${env:AUTO_EXPLAIN_LOG_TIMING:-true}
      log_triggers: ${env:AUTO_EXPLAIN_LOG_TRIGGERS:-true}
      log_verbose: ${env:AUTO_EXPLAIN_LOG_VERBOSE:-false}

  # Sophisticated PostgreSQL monitoring queries
  sqlquery/plan-intelligence:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:-localhost} port=${env:POSTGRES_PORT:-5432} user=${env:POSTGRES_USER:-postgres} password=${env:POSTGRES_PASSWORD:-postgres} dbname=${env:POSTGRES_DB:-postgres} sslmode=disable"
    collection_interval: ${env:PLAN_QUERY_INTERVAL:-60s}
    queries:
      # Advanced pg_stat_statements with plan information
      - sql: |
          SELECT 
            queryid,
            query,
            calls,
            total_exec_time,
            mean_exec_time,
            stddev_exec_time,
            rows,
            100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent,
            blk_read_time,
            blk_write_time,
            temp_blks_read,
            temp_blks_written,
            wal_records,
            wal_fpi,
            wal_bytes
          FROM pg_stat_statements
          WHERE calls > 10
          ORDER BY total_exec_time DESC
          LIMIT 100
        metrics:
          - metric_name: pg_stat_statements_advanced
            value_column: total_exec_time
            attribute_columns: [queryid, calls, mean_exec_time, hit_percent]

      # Query plan regression detection
      - sql: |
          WITH plan_changes AS (
            SELECT 
              s.queryid,
              s.query,
              s.calls,
              s.total_exec_time / s.calls as current_avg_time,
              LAG(s.total_exec_time / s.calls) OVER (
                PARTITION BY s.queryid 
                ORDER BY stats_reset
              ) as previous_avg_time
            FROM pg_stat_statements s
            WHERE s.calls > 5
          )
          SELECT 
            queryid,
            query,
            current_avg_time,
            previous_avg_time,
            CASE 
              WHEN previous_avg_time > 0 THEN 
                (current_avg_time - previous_avg_time) / previous_avg_time * 100
              ELSE 0 
            END as performance_change_percent
          FROM plan_changes
          WHERE previous_avg_time IS NOT NULL
            AND ABS((current_avg_time - previous_avg_time) / previous_avg_time) > 0.2
        metrics:
          - metric_name: plan_regression_detection
            value_column: performance_change_percent
            attribute_columns: [queryid, current_avg_time, previous_avg_time]

      # Wait event analysis
      - sql: |
          SELECT 
            wait_event_type,
            wait_event,
            COUNT(*) as wait_count,
            AVG(EXTRACT(EPOCH FROM (clock_timestamp() - query_start))) as avg_wait_time
          FROM pg_stat_activity
          WHERE state = 'active' 
            AND wait_event IS NOT NULL
          GROUP BY wait_event_type, wait_event
          ORDER BY wait_count DESC
        metrics:
          - metric_name: wait_event_analysis
            value_column: wait_count
            attribute_columns: [wait_event_type, wait_event, avg_wait_time]

      # I/O timing breakdown
      - sql: |
          SELECT 
            datname as database_name,
            blks_read,
            blks_hit,
            blk_read_time,
            blk_write_time,
            temp_files,
            temp_bytes,
            100.0 * blks_hit / NULLIF(blks_hit + blks_read, 0) AS cache_hit_ratio
          FROM pg_stat_database
          WHERE datname NOT IN ('template0', 'template1', 'postgres')
        metrics:
          - metric_name: database_io_timing
            value_column: cache_hit_ratio
            attribute_columns: [database_name, blks_read, blk_read_time, blk_write_time]

# Advanced processors for plan intelligence
processors:
  # Enhanced plan attribute extractor
  planattributeextractor:
    enable_anonymization: ${env:ENABLE_ANONYMIZATION:-true}
    enable_plan_analysis: true
    max_query_length: ${env:MAX_QUERY_LENGTH:-8192}
    plan_intelligence:
      enabled: ${env:ENABLE_PLAN_INTELLIGENCE:-true}
      regression_detection:
        enabled: ${env:ENABLE_REGRESSION_DETECTION:-true}
        time_threshold: ${env:REGRESSION_TIME_THRESHOLD:-1.5}
        io_threshold: ${env:REGRESSION_IO_THRESHOLD:-2.0}
        memory_threshold: ${env:REGRESSION_MEMORY_THRESHOLD:-1.8}
        cpu_threshold: ${env:REGRESSION_CPU_THRESHOLD:-1.6}
        min_execution_count: ${env:MIN_EXECUTION_COUNT:-10}
      plan_optimization:
        enabled: ${env:ENABLE_PLAN_OPTIMIZATION:-true}
        suggest_indexes: ${env:SUGGEST_INDEXES:-true}
        detect_scans: ${env:DETECT_SCANS:-true}
        analyze_joins: ${env:ANALYZE_JOINS:-true}
      plan_cache:
        enabled: true
        max_size: ${env:PLAN_CACHE_MAX_SIZE:-20000}
        ttl: ${env:PLAN_CACHE_TTL:-7200s}
        eviction_policy: ${env:PLAN_CACHE_EVICTION:-lru}
    query_fingerprinting:
      enabled: ${env:ENABLE_FINGERPRINTING:-true}
      normalize_parameters: true
      preserve_comments: false
      hash_algorithm: ${env:HASH_ALGORITHM:-sha256}

  # Wait analysis processor
  waitanalysis:
    enabled: ${env:ENABLE_WAIT_ANALYSIS:-true}
    wait_event_grouping:
      enabled: ${env:ENABLE_WAIT_GROUPING:-true}
      group_threshold: ${env:WAIT_GROUP_THRESHOLD:-10}
    contention_detection:
      enabled: ${env:ENABLE_CONTENTION_DETECTION:-true}
      lock_timeout_threshold: ${env:LOCK_TIMEOUT_THRESHOLD:-5000}
      deadlock_detection: ${env:ENABLE_DEADLOCK_DETECTION:-true}

# Plan intelligence service configuration
service:
  extensions: [health_check, pprof, zpages, memory_ballast, filestorage]
  pipelines:
    # Plan intelligence metrics pipeline
    metrics/plan-intelligence:
      receivers: [
        postgresql, 
        sqlquery/plan-intelligence, 
        enhancedsql/plan-intelligence,
        ash
      ]
      processors: [
        memory_limiter,
        planattributeextractor,
        waitanalysis,
        verification,
        resource,
        attributes,
        batch
      ]
      exporters: [otlphttp/newrelic, prometheus, file]
    
    # Plan intelligence traces pipeline
    traces/plan-intelligence:
      receivers: [enhancedsql/plan-intelligence, ash]
      processors: [
        memory_limiter,
        planattributeextractor,
        waitanalysis,
        resource,
        attributes,
        batch
      ]
      exporters: [otlphttp/newrelic, jaeger]

# Plan intelligence specific exporters
exporters:
  # File exporter for plan history
  file/plan-history:
    path: ${env:PLAN_HISTORY_PATH:-/var/log/otel/plan-history.jsonl}
    rotation:
      max_megabytes: ${env:PLAN_HISTORY_MAX_MB:-100}
      max_days: ${env:PLAN_HISTORY_MAX_DAYS:-30}
      max_backups: ${env:PLAN_HISTORY_MAX_BACKUPS:-10}
    format: json

  # Specialized New Relic export for plan data
  otlphttp/newrelic-plans:
    endpoint: ${env:NEW_RELIC_OTLP_ENDPOINT:-https://otlp.nr-data.net}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
      nr-data-type: plan-intelligence
    compression: gzip
    timeout: ${env:NR_PLANS_TIMEOUT:-45s}