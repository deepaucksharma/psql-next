# Base Collector Configuration
# This is the foundation configuration that all environments build upon

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
    path: /
    check_collector_pipeline:
      enabled: true
      interval: 15s
      exporter_failure_threshold: 5
  
  pprof:
    endpoint: 0.0.0.0:1777
    block_profile_fraction: 0
    mutex_profile_fraction: 0
  
  zpages:
    endpoint: 0.0.0.0:55679

receivers:
  # PostgreSQL receiver configuration
  postgresql:
    endpoint: ${env:POSTGRES_HOST}:${env:POSTGRES_PORT}
    username: ${env:POSTGRES_USER}
    password: ${env:POSTGRES_PASSWORD}
    databases:
      - ${env:POSTGRES_DB}
    collection_interval: 60s
    initial_delay: 30s
    timeout: 30s
    tls:
      insecure: true
    
  # MySQL receiver configuration  
  mysql:
    endpoint: ${env:MYSQL_HOST}:${env:MYSQL_PORT}
    username: ${env:MYSQL_USER}
    password: ${env:MYSQL_PASSWORD}
    database: ${env:MYSQL_DB}
    collection_interval: 60s
    initial_delay: 30s
    
  # SQL query receiver for custom metrics
  sqlquery:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST} port=${env:POSTGRES_PORT} user=${env:POSTGRES_USER} password=${env:POSTGRES_PASSWORD} dbname=${env:POSTGRES_DB} sslmode=disable"
    collection_interval: 300s
    queries:
      - query: |
          SELECT 
            current_database() as db,
            count(*) as query_count,
            sum(calls) as total_calls,
            avg(mean_exec_time) as avg_duration
          FROM pg_stat_statements
        metrics:
          - metric_name: db.query.stats
            value_column: query_count
            value_type: int
            attribute_columns: [db]
          - metric_name: db.query.calls
            value_column: total_calls
            value_type: int
            attribute_columns: [db]
          - metric_name: db.query.avg_duration
            value_column: avg_duration
            value_type: double
            unit: ms
            attribute_columns: [db]

processors:
  # Memory limiter prevents OOM
  memory_limiter:
    check_interval: 1s
    limit_percentage: 75
    spike_limit_percentage: 25
    
  # Resource processor adds metadata
  resource:
    attributes:
      - key: service.name
        value: database-intelligence
        action: upsert
      - key: service.namespace
        value: monitoring
        action: upsert
      - key: deployment.environment
        value: ${env:ENVIRONMENT}
        action: upsert
        
  # Batch processor for efficiency
  batch:
    timeout: 10s
    send_batch_size: 1024
    send_batch_max_size: 2048
    
  # Filter processor for data reduction
  filter/errors:
    error_mode: ignore
    metrics:
      metric:
        - 'name == "db.errors" and attributes["severity"] != "critical"'

exporters:
  # OTLP exporter to New Relic
  otlp/newrelic:
    endpoint: ${env:NEW_RELIC_OTLP_ENDPOINT}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
      
  # Prometheus exporter for local metrics
  prometheus:
    endpoint: 0.0.0.0:8888
    namespace: otelcol
    const_labels:
      environment: ${env:ENVIRONMENT}
    resource_to_telemetry_conversion:
      enabled: true
      
  # Debug exporter (disabled by default)
  debug:
    verbosity: basic
    sampling_initial: 5
    sampling_thereafter: 200

service:
  telemetry:
    logs:
      level: info
      encoding: json
      output_paths: ["stdout"]
      error_output_paths: ["stderr"]
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
  extensions: [health_check, pprof, zpages]
  
  pipelines:
    metrics/infrastructure:
      receivers: [postgresql, mysql]
      processors: [memory_limiter, resource, batch]
      exporters: [otlp/newrelic, prometheus]
      
    metrics/queries:
      receivers: [sqlquery]
      processors: [memory_limiter, resource, filter/errors, batch]
      exporters: [otlp/newrelic]