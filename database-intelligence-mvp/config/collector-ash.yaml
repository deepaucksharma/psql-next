# Active Session History (ASH) Collector Configuration
# This configuration enables high-frequency session sampling for performance analysis

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
    
  zpages:
    endpoint: 0.0.0.0:55679

receivers:
  # Standard PostgreSQL receiver for baseline metrics
  postgresql:
    endpoint: ${env:POSTGRES_HOST:-localhost}:${env:POSTGRES_PORT:-5432}
    username: ${env:POSTGRES_USER:-postgres}
    password: ${env:POSTGRES_PASSWORD:-postgres}
    databases:
      - ${env:POSTGRES_DB:-postgres}
    tls:
      insecure: true
    collection_interval: 60s

  # ASH receiver for session history
  ash:
    endpoint: ${env:POSTGRES_HOST:-localhost}:${env:POSTGRES_PORT:-5432}
    username: ${env:POSTGRES_USER:-postgres}
    password: ${env:POSTGRES_PASSWORD:-postgres}
    database: ${env:POSTGRES_DB:-postgres}
    
    # ASH collection settings
    collection_interval: 1s        # Standard ASH sampling interval
    retention_duration: 1h         # Keep 1 hour of detailed data
    
    # Adaptive sampling configuration
    sampling:
      enabled: true
      sample_rate: 1.0             # Base sample rate (100%)
      active_session_rate: 1.0     # Sample all active sessions
      blocked_session_rate: 1.0    # Sample all blocked sessions
      long_running_threshold: 10s  # Mark queries > 10s as long-running
      adaptive_sampling: true      # Reduce sampling under high load
    
    # Storage configuration
    storage:
      buffer_size: 3600            # 1 hour at 1 sample/second
      aggregation_windows:
        - 1m                       # 1-minute aggregations
        - 5m                       # 5-minute aggregations
        - 15m                      # 15-minute aggregations
        - 1h                       # 1-hour aggregations
      compression_enabled: true    # Enable snapshot compression
    
    # Analysis features
    analysis:
      wait_event_analysis: true    # Categorize and analyze wait events
      blocking_analysis: true      # Detect blocking chains
      resource_analysis: true      # Track resource usage patterns
      anomaly_detection: true      # Detect unusual patterns
      top_query_analysis: true     # Identify top resource consumers
      trend_analysis: true         # Track performance trends
    
    # Feature detection
    feature_detection:
      enabled: true
      check_interval: 5m
      required_extensions:
        - pg_stat_statements       # For query identification
        - pg_wait_sampling         # For detailed wait events (optional)

  # Enhanced SQL receiver for ASH-aware queries
  enhancedsql/ash_queries:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:-localhost} port=${env:POSTGRES_PORT:-5432} user=${env:POSTGRES_USER:-postgres} password=${env:POSTGRES_PASSWORD:-postgres} dbname=${env:POSTGRES_DB:-postgres} sslmode=disable"
    
    collection_interval: 60s
    
    queries:
      # Query for wait event summary
      - sql: |
          SELECT 
            wait_event_type,
            wait_event,
            count(*) as session_count,
            array_agg(pid) as pids
          FROM pg_stat_activity
          WHERE state != 'idle'
            AND wait_event IS NOT NULL
          GROUP BY wait_event_type, wait_event
          ORDER BY session_count DESC
          LIMIT 20
        metrics:
          - metric_name: db.postgresql.ash.wait_summary
            value_column: session_count
            value_type: gauge
            attribute_columns: [wait_event_type, wait_event]
      
      # Query for blocking analysis
      - sql: |
          WITH blocking_tree AS (
            SELECT 
              blocked.pid AS blocked_pid,
              blocked.usename AS blocked_user,
              blocked.query AS blocked_query,
              blocking.pid AS blocking_pid,
              blocking.usename AS blocking_user,
              blocking.query AS blocking_query,
              blocked.wait_event_type,
              blocked.wait_event
            FROM pg_stat_activity blocked
            JOIN pg_stat_activity blocking 
              ON blocking.pid = ANY(pg_blocking_pids(blocked.pid))
            WHERE blocked.wait_event_type = 'Lock'
          )
          SELECT 
            blocking_pid,
            blocking_user,
            count(*) as blocked_count,
            array_agg(blocked_pid) as blocked_pids
          FROM blocking_tree
          GROUP BY blocking_pid, blocking_user, blocking_query
          ORDER BY blocked_count DESC
        metrics:
          - metric_name: db.postgresql.ash.blocking_sessions
            value_column: blocked_count
            value_type: gauge
            attribute_columns: [blocking_pid, blocking_user]

processors:
  # Memory limiter
  memory_limiter:
    check_interval: 1s
    limit_percentage: 80
    spike_limit_percentage: 25

  # Add standard attributes
  resource:
    attributes:
      - key: service.name
        value: database-intelligence-ash
        action: upsert
      - key: deployment.environment
        value: ${env:ENVIRONMENT:-development}
        action: upsert

  # Wait event processor - categorizes and enriches wait events
  waitanalysis:
    enabled: true
    
    # Wait event patterns for categorization
    patterns:
      - name: lock_waits
        event_types: ["Lock"]
        category: "Concurrency"
        severity: "warning"
        
      - name: io_waits
        event_types: ["IO"]
        events: ["DataFileRead", "DataFileWrite", "WALWrite"]
        category: "Storage"
        severity: "info"
        
      - name: cpu_waits
        event_types: ["CPU"]
        category: "Compute"
        severity: "info"
        
      - name: network_waits
        event_types: ["Client", "IPC"]
        category: "Network"
        severity: "info"
    
    # Alert rules based on wait events
    alert_rules:
      - name: excessive_lock_waits
        condition: "wait_time > 5s AND event_type = 'Lock'"
        threshold: 10  # More than 10 sessions
        window: 1m
        action: alert
        
      - name: io_saturation
        condition: "event IN ('DataFileRead', 'DataFileWrite') AND wait_time > 100ms"
        threshold: 50  # More than 50% of sessions
        window: 5m
        action: alert

  # Session anomaly detector
  anomalydetector/sessions:
    enabled: true
    
    # Anomaly detection rules
    rules:
      - name: session_spike
        metric: session_count
        method: stddev
        threshold: 3  # 3 standard deviations
        window: 5m
        
      - name: blocking_chain
        metric: blocking_depth
        method: threshold
        threshold: 3  # Blocking chain > 3 levels
        
      - name: wait_event_storm
        metric: wait_event_count
        method: rate_change
        threshold: 5  # 5x increase
        window: 1m

  # Batch processor
  batch:
    timeout: 10s
    send_batch_size: 1000

exporters:
  # OTLP exporter to New Relic
  otlp/newrelic:
    endpoint: ${env:NEW_RELIC_OTLP_ENDPOINT:-otlp.nr-data.net:4317}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 60s

  # Prometheus for local metrics
  prometheus:
    endpoint: 0.0.0.0:8889
    namespace: ash
    const_labels:
      environment: ${env:ENVIRONMENT:-development}
    
    # ASH-specific metric renaming
    metric_expiration: 5m  # Expire inactive series quickly

  # Debug exporter for development
  debug:
    verbosity: detailed
    sampling_initial: 10
    sampling_thereafter: 100

service:
  extensions: [health_check, zpages]
  
  pipelines:
    # ASH metrics pipeline
    metrics/ash:
      receivers: [ash]
      processors: [memory_limiter, resource, waitanalysis, anomalydetector/sessions, batch]
      exporters: [otlp/newrelic, prometheus]
    
    # Infrastructure metrics
    metrics/infrastructure:
      receivers: [postgresql]
      processors: [memory_limiter, resource, batch]
      exporters: [otlp/newrelic, prometheus]
    
    # ASH analysis queries
    metrics/ash_analysis:
      receivers: [enhancedsql/ash_queries]
      processors: [memory_limiter, resource, batch]
      exporters: [otlp/newrelic, prometheus]
      
  telemetry:
    logs:
      level: ${env:LOG_LEVEL:-info}
      encoding: json
    metrics:
      level: detailed
      address: 0.0.0.0:8889