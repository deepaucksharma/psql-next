extensions:
  health_check:
    endpoint: 0.0.0.0:13133
    
  zpages:
    endpoint: 0.0.0.0:55679


receivers:
  # Standard OTEL receivers for database metrics
  postgresql:
    endpoint: ${env:POSTGRES_HOST:-localhost}:${env:POSTGRES_PORT:-5432}
    username: ${env:POSTGRES_USER:-postgres}
    password: ${env:POSTGRES_PASSWORD:-postgres}
    databases:
      - ${env:POSTGRES_DB:-postgres}
    tls:
      insecure: true
    collection_interval: 60s

  mysql:
    endpoint: ${env:MYSQL_HOST:-localhost}:${env:MYSQL_PORT:-3306}
    username: ${env:MYSQL_USER:-root}
    password: ${env:MYSQL_PASSWORD:-mysql}
    database: ${env:MYSQL_DB:-mysql}
    collection_interval: 60s

  # SQL query receiver for custom query statistics
  sqlquery/postgresql:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:-localhost} port=${env:POSTGRES_PORT:-5432} user=${env:POSTGRES_USER:-postgres} password=${env:POSTGRES_PASSWORD:-postgres} dbname=${env:POSTGRES_DB:-postgres} sslmode=disable"
    collection_interval: 300s
    queries:
      # Feature detection query
      - sql: |
          -- Safety timeouts to prevent long-running queries
          SET LOCAL statement_timeout = '3000ms';
          SET LOCAL lock_timeout = '100ms';

          -- Check available extensions
          SELECT 
            extname as extension_name,
            extversion as version,
            1 as available
          FROM pg_extension
          WHERE extname IN ('pg_stat_statements', 'pg_stat_monitor', 'pg_wait_sampling', 'auto_explain')
          UNION ALL
          SELECT 
            name as extension_name,
            default_version as version,
            0 as available
          FROM pg_available_extensions
          WHERE name IN ('pg_stat_statements', 'pg_stat_monitor', 'pg_wait_sampling')
            AND installed_version IS NULL
        metrics:
          - metric_name: db.extension.available
            value_column: available
            value_type: int
            attribute_columns: [extension_name, version]
            
      # Slow queries with fallback support
      - sql: |
          -- Try pg_stat_statements first (most common)
          WITH feature_check AS (
            SELECT EXISTS (
              SELECT 1 FROM pg_extension WHERE extname = 'pg_stat_statements'
            ) as has_pg_stat_statements
          )
          SELECT
            CASE 
              WHEN fc.has_pg_stat_statements THEN pss.queryid::text
              ELSE psa.pid::text
            END as query_id,
            COALESCE(pss.query, psa.query) as query_text,
            CASE
              WHEN fc.has_pg_stat_statements THEN round(pss.mean_exec_time::numeric, 2)
              ELSE EXTRACT(EPOCH FROM (now() - psa.query_start)) * 1000
            END as avg_duration_ms,
            COALESCE(pss.calls, 1) as execution_count,
            CASE
              WHEN fc.has_pg_stat_statements THEN round(pss.total_exec_time::numeric, 2)
              ELSE EXTRACT(EPOCH FROM (now() - psa.query_start)) * 1000
            END as total_duration_ms,
            current_database() as database_name
          FROM feature_check fc
          LEFT JOIN pg_stat_statements pss ON fc.has_pg_stat_statements
          LEFT JOIN pg_stat_activity psa ON NOT fc.has_pg_stat_statements 
            AND psa.state = 'active' 
            AND psa.query_start < now() - interval '1 second'
          WHERE (fc.has_pg_stat_statements AND pss.mean_exec_time > 50)
            OR (NOT fc.has_pg_stat_statements AND psa.pid IS NOT NULL)
          ORDER BY avg_duration_ms DESC
          LIMIT 10
        logs:
          - body_column: query_text
            attributes:
              query_id: query_id
              avg_duration_ms: avg_duration_ms
              execution_count: execution_count
              total_duration_ms: total_duration_ms
              database_name: database_name

  sqlquery/mysql:
    driver: mysql
    datasource: "${env:MYSQL_USER:-root}:${env:MYSQL_PASSWORD:-mysql}@tcp(${env:MYSQL_HOST:-localhost}:${env:MYSQL_PORT:-3306})/${env:MYSQL_DB:-mysql}"
    collection_interval: 300s
    queries:
      - sql: |
          SELECT
            DIGEST as query_id,
            DIGEST_TEXT as query_text,
            ROUND(AVG_TIMER_WAIT/1000000, 2) as avg_duration_ms,
            COUNT_STAR as execution_count,
            ROUND((AVG_TIMER_WAIT * COUNT_STAR)/1000000, 2) as total_duration_ms,
            SCHEMA_NAME as database_name
          FROM performance_schema.events_statements_summary_by_digest
          WHERE SCHEMA_NAME IS NOT NULL
            AND AVG_TIMER_WAIT > 50000000
            AND COUNT_STAR > 5
          ORDER BY AVG_TIMER_WAIT DESC
          LIMIT 10
        logs:
          - body_column: query_text
            attributes:
              query_id: query_id
              avg_duration_ms: avg_duration_ms
              execution_count: execution_count
              total_duration_ms: total_duration_ms
              database_name: database_name

processors:
  memory_limiter:
    check_interval: 2s
    limit_mib: 1024
    spike_limit_mib: 256

  # Add collector.name attribute for dashboard queries
  resource:
    attributes:
      - key: collector.name
        value: otelcol
        action: upsert
      - key: collector.instance.id
        value: ${env:HOSTNAME}
        action: upsert

  # Transform to ensure numeric values are properly typed
  transform/metrics:
    error_mode: ignore
    metric_statements:
      - context: metric
        statements:
          - set(unit, "1") where name == "pg_stat_statements.available"

  transform/logs:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Ensure numeric fields are numbers
          - set(attributes["avg_duration_ms"], Double(attributes["avg_duration_ms"])) where attributes["avg_duration_ms"] != nil
          - set(attributes["execution_count"], Int(attributes["execution_count"])) where attributes["execution_count"] != nil
          - set(attributes["total_duration_ms"], Double(attributes["total_duration_ms"])) where attributes["total_duration_ms"] != nil
          # Set collector.name if not present
          - set(attributes["collector.name"], "otelcol") where attributes["collector.name"] == nil

  # PII sanitization
  transform/sanitize_pii:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - replace_all_patterns(attributes["query_text"], "'[^']*'", "'[REDACTED]'")
          - replace_all_patterns(attributes["query_text"], "\\b\\d{6,}\\b", "[ID]")
          - replace_all_patterns(body, "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b", "[EMAIL_REDACTED]")

  probabilistic_sampler:
    hash_seed: 22
    sampling_percentage: 25

  batch:
    timeout: 30s
    send_batch_size: 50
    send_batch_max_size: 100

exporters:
  debug:
    verbosity: detailed
    sampling_initial: 10
    sampling_thereafter: 100

  otlp/newrelic:
    endpoint: ${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4317}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 120s
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 256

service:
  extensions: [health_check, zpages]
  pipelines:
    metrics/databases:
      receivers: [postgresql, mysql]
      processors: [memory_limiter, resource, transform/metrics, batch]
      exporters: [debug, otlp/newrelic]
    
    metrics/queries:
      receivers: [sqlquery/postgresql, sqlquery/mysql]
      processors: [memory_limiter, resource, transform/metrics, batch]
      exporters: [debug, otlp/newrelic]
    
    logs/queries:
      receivers: [sqlquery/postgresql, sqlquery/mysql]
      processors: [memory_limiter, resource, transform/logs, transform/sanitize_pii, probabilistic_sampler, batch]
      exporters: [debug, otlp/newrelic]
      
  telemetry:
    logs:
      level: info
      encoding: json
    metrics:
      level: detailed
      address: 0.0.0.0:8888