# OpenTelemetry Collector Configuration for Database Intelligence
#
# This is the unified, production-ready configuration for the v1.0.0 release.
# It is designed for a high-availability deployment using the leader_election
# extension in Kubernetes.

extensions:
  health_check:
    endpoint: 0.0.0.0:13133

  leader_election:
    lease_name: db-intelligence-leader
    lease_namespace: db-intelligence # This should match the namespace of your deployment

  memory_ballast:
    size_mib: 128

receivers:
  # PostgreSQL receiver for metadata collection
  sqlquery/postgresql:
    driver: postgres
    dsn: "${env:PG_REPLICA_DSN}"
    collection_interval: 300s
    timeout: 10s
    max_open_connections: 2
    max_idle_connections: 1
    connection_max_lifetime: 300s
    enabled_when_leader: true # Only the leader collects data
    queries:
      - sql: |
          -- Safety timeouts to prevent long-running queries
          SET LOCAL statement_timeout = '3000ms';
          SET LOCAL lock_timeout = '100ms';

          -- Find the worst-performing query based on total execution time
          WITH worst_query AS (
            SELECT
              queryid,
              query,
              mean_exec_time,
              calls,
              total_exec_time,
              (mean_exec_time * calls) as impact_score
            FROM pg_stat_statements
            WHERE
              mean_exec_time > 50        -- Only consider queries over 50ms
              AND calls > 5              -- And those that are somewhat frequent
              AND query NOT LIKE '%pg_%' -- Exclude system queries
              AND query NOT LIKE '%EXPLAIN%'
              AND query NOT LIKE '%SET LOCAL%'
            ORDER BY impact_score DESC
            LIMIT 1
          )
          SELECT
            queryid::text as query_id,
            query as query_text,
            round(mean_exec_time::numeric, 2) as avg_duration_ms,
            calls as execution_count,
            round(total_exec_time::numeric, 2) as total_duration_ms,
            -- This is a placeholder for the plan, as we are not collecting actual plans
            json_build_object(
              'plan_available', false,
              'approach', 'metadata_only_for_safety'
            ) as plan_metadata
          FROM worst_query;

  # MySQL receiver for metadata collection
  sqlquery/mysql:
    driver: mysql
    dsn: "${env:MYSQL_READONLY_DSN}"
    collection_interval: 300s
    timeout: 10s
    max_open_connections: 2
    max_idle_connections: 1
    connection_max_lifetime: 300s
    enabled_when_leader: true # Only the leader collects data
    queries:
      - sql: |
          SELECT
            DIGEST as query_id,
            DIGEST_TEXT as query_text,
            ROUND(AVG_TIMER_WAIT/1000000, 2) as avg_duration_ms,
            COUNT_STAR as execution_count,
            ROUND((AVG_TIMER_WAIT * COUNT_STAR)/1000000, 2) as total_duration_ms,
            JSON_OBJECT(
              'plan_available', false,
              'approach', 'performance_schema_metadata'
            ) as plan_metadata
          FROM performance_schema.events_statements_summary_by_digest
          WHERE
            SCHEMA_NAME = DATABASE()
            AND SCHEMA_NAME IS NOT NULL
            AND AVG_TIMER_WAIT > 50000000  -- 50ms+
            AND COUNT_STAR > 5
          ORDER BY (AVG_TIMER_WAIT * COUNT_STAR) DESC
          LIMIT 1;

processors:
  memory_limiter:
    check_interval: 2s
    limit_mib: 1024
    spike_limit_mib: 256

  transform/sanitize_pii:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Redact sensitive information from query text
          - replace_all_patterns(attributes["query_text"], "'[^']*'", "'[REDACTED]'")
          - replace_all_patterns(attributes["query_text"], "\\b\\d{6,}\\b", "[ID]")
          - replace_all_patterns(body, "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b", "[EMAIL_REDACTED]")

  # Add metadata to identify the collector instance
  resource:
    attributes:
      - key: "collector.instance.id"
        value: "${env:HOSTNAME}"
        action: upsert

  probabilistic_sampler:
    hash_seed: 22
    sampling_percentage: 25

  batch:
    timeout: 30s
    send_batch_size: 50
    send_batch_max_size: 100

exporters:
  otlp/newrelic:
    endpoint: "${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4317}"
    headers:
      api-key: "${env:NEW_RELIC_LICENSE_KEY}"
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 120s
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 256

service:
  extensions: [health_check, leader_election, memory_ballast]
  pipelines:
    logs/database_intelligence:
      receivers: [sqlquery/postgresql, sqlquery/mysql]
      processors: [memory_limiter, transform/sanitize_pii, resource, probabilistic_sampler, batch]
      exporters: [otlp/newrelic]
  telemetry:
    logs:
      level: info
      encoding: json
    metrics:
      level: detailed
      address: 0.0.0.0:8888
