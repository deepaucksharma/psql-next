# Secure OpenTelemetry Collector Configuration
# This is the main secure configuration for the Database Intelligence Collector

extensions:
  # Health check with restricted access
  health_check:
    endpoint: "127.0.0.1:13133"  # Localhost only for security
    path: "/health"
    check_collector_pipeline:
      enabled: true
      interval: 30s
      exporter_failure_threshold: 3
      
  # zpages disabled for security in production environments
  # zpages:
  #   endpoint: disabled

receivers:
  # PostgreSQL receiver with security hardening
  postgresql:
    endpoint: "${env:POSTGRES_HOST:-localhost}:${env:POSTGRES_PORT:-5432}"
    username: "${env:POSTGRES_USER:-postgres}"
    password: "${env:POSTGRES_PASSWORD:-devpassword123}"
    databases:
      - "${env:POSTGRES_DB:-postgres}"
    collection_interval: 60s
    timeout: 30s  # Reasonable timeout to prevent hanging
    tls:
      insecure: "${env:POSTGRES_TLS_INSECURE:-true}"
      insecure_skip_verify: "${env:POSTGRES_TLS_SKIP_VERIFY:-true}"
      # In production, configure proper TLS:
      # insecure: false
      # cert_file: "/etc/ssl/certs/postgres-client.crt"
      # key_file: "/etc/ssl/private/postgres-client.key"
      # ca_file: "/etc/ssl/certs/postgres-ca.crt"
    metrics:
      # Enable only necessary metrics to reduce cardinality
      postgresql.blocks_read:
        enabled: true
      postgresql.blocks_written:
        enabled: true
      postgresql.commits:
        enabled: true
      postgresql.rollbacks:
        enabled: true
      postgresql.database.count:
        enabled: true
      postgresql.database.locks:
        enabled: true
      postgresql.database.backends:
        enabled: true
      postgresql.bgwriter.buffers.allocated:
        enabled: true
      postgresql.bgwriter.buffers.writes:
        enabled: true
      postgresql.bgwriter.checkpoint.count:
        enabled: true
      postgresql.bgwriter.duration:
        enabled: true
      # Disable high-cardinality metrics
      postgresql.table.size:
        enabled: false
      postgresql.table.vacuum.count:
        enabled: false
      postgresql.index.size:
        enabled: false

  # MySQL receiver with security hardening  
  mysql:
    endpoint: "${env:MYSQL_HOST:-localhost}:${env:MYSQL_PORT:-3306}"
    username: "${env:MYSQL_USER:-root}"
    password: "${env:MYSQL_PASSWORD:-devmysqlpass123}"
    database: "${env:MYSQL_DB:-mysql}"
    collection_interval: 60s
    timeout: 30s
    tls:
      insecure: "${env:MYSQL_TLS_INSECURE:-true}"
      insecure_skip_verify: "${env:MYSQL_TLS_SKIP_VERIFY:-true}"
      # In production, configure proper TLS:
      # insecure: false  
      # cert_file: "/etc/ssl/certs/mysql-client.crt"
      # key_file: "/etc/ssl/private/mysql-client.key"
      # ca_file: "/etc/ssl/certs/mysql-ca.crt"
    metrics:
      # Enable only essential metrics
      mysql.connections:
        enabled: true
      mysql.operations:
        enabled: true
      mysql.handlers:
        enabled: true
      mysql.locks:
        enabled: true
      mysql.buffer_pool:
        enabled: true
      mysql.innodb:
        enabled: true
      # Disable potentially problematic metrics
      mysql.table_io_wait:
        enabled: false
      mysql.index_io_wait:
        enabled: false

  # Secure SQL query receiver for feature detection
  sqlquery/postgresql_secure:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:-localhost} port=${env:POSTGRES_PORT:-5432} user=${env:POSTGRES_USER:-postgres} password=${env:POSTGRES_PASSWORD:-devpassword123} dbname=${env:POSTGRES_DB:-postgres} sslmode=${env:POSTGRES_SSL_MODE:-disable} connect_timeout=10"
    collection_interval: 300s  # 5 minutes for feature detection
    timeout: 60s               # Query timeout for safety
    queries:
      # Secure feature detection query
      - sql: |
          -- Security: Set strict timeouts
          SET LOCAL statement_timeout = '30s';
          SET LOCAL lock_timeout = '5s';
          SET LOCAL idle_in_transaction_session_timeout = '60s';
          
          -- Feature detection with security considerations
          WITH extension_check AS (
            SELECT 
              extname as extension_name,
              extversion as version,
              'installed' as status
            FROM pg_extension
            WHERE extname IN ('pg_stat_statements', 'pg_stat_monitor', 'pg_wait_sampling')
          ),
          available_extensions AS (
            SELECT 
              name as extension_name,
              default_version as version,
              'available' as status
            FROM pg_available_extensions
            WHERE name IN ('pg_stat_statements', 'pg_stat_monitor', 'pg_wait_sampling')
          )
          SELECT 
            COALESCE(e.extension_name, a.extension_name) as extension_name,
            COALESCE(e.version, a.version) as version,
            CASE 
              WHEN e.status = 'installed' THEN 'installed'
              WHEN a.status = 'available' THEN 'available'
              ELSE 'unavailable'
            END as availability_status,
            CURRENT_TIMESTAMP as check_timestamp
          FROM extension_check e
          FULL OUTER JOIN available_extensions a ON e.extension_name = a.extension_name
          ORDER BY extension_name;
        metrics:
          - metric_name: postgresql.extension.available
            value_column: "availability_status"
            attribute_columns: ["extension_name", "version"]
            data_type: gauge
            value_type: int
            
      # Secure performance statistics query
      - sql: |
          SET LOCAL statement_timeout = '15s';
          SET LOCAL lock_timeout = '2s';
          
          SELECT 
            datname as database_name,
            numbackends as active_connections,
            xact_commit as committed_transactions,
            xact_rollback as rolled_back_transactions,
            blks_read as blocks_read,
            blks_hit as blocks_hit,
            CASE 
              WHEN (blks_read + blks_hit) > 0 
              THEN ROUND((blks_hit::float / (blks_read + blks_hit)) * 100, 2)
              ELSE 0 
            END as cache_hit_ratio,
            CURRENT_TIMESTAMP as measurement_time
          FROM pg_stat_database 
          WHERE datname NOT IN ('template0', 'template1', 'postgres')
          AND datname = current_database();
        metrics:
          - metric_name: postgresql.database.connections.active
            value_column: "active_connections"
            attribute_columns: ["database_name"]
            data_type: gauge
            value_type: int
          - metric_name: postgresql.database.transactions.committed
            value_column: "committed_transactions"
            attribute_columns: ["database_name"]
            data_type: gauge
            value_type: int
          - metric_name: postgresql.database.cache_hit_ratio
            value_column: "cache_hit_ratio"
            attribute_columns: ["database_name"]
            data_type: gauge
            value_type: double

processors:
  # Memory limiter is critical for security and stability
  memory_limiter:
    check_interval: 5s
    limit_mib: 512
    spike_limit_mib: 128
    
  # Resource attribution for proper identification
  resource:
    attributes:
      - key: collector.name
        value: "database-intelligence-secure"
        action: upsert
      - key: service.name  
        value: "database-intelligence"
        action: upsert
      - key: service.namespace
        value: "${env:ENVIRONMENT:-development}"
        action: upsert
      - key: service.version
        value: "1.0.0"
        action: upsert
      - key: deployment.environment
        value: "${env:ENVIRONMENT:-development}"
        action: upsert
      - key: security.enabled
        value: "true"
        action: upsert
      - key: collector.version
        value: "secure-1.0.0"
        action: upsert
        
  # Circuit breaker for resilience and security
  circuitbreaker:
    failure_threshold: 5
    success_threshold: 3
    open_state_timeout: 60s
    max_concurrent_requests: 25
    enable_adaptive_timeout: true
    base_timeout: 10s
    max_timeout: 60s
    memory_threshold_mb: 256
    cpu_threshold_percent: 85
    health_check_interval: 30s
    new_relic_error_patterns:
      - "rate limit"
      - "cardinality"
      - "quota exceeded"
      - "forbidden"
      - "unauthorized"
    
  # Plan attribute extraction with security
  planattributeextractor:
    safe_mode: true
    timeout_ms: 5000
    error_mode: ignore
    enable_debug_logging: false
    hash_config:
      algorithm: "sha256"  # Only secure hash algorithm
      include:
        - "db.query.plan.cost"
        - "db.query.plan.rows"
        - "db.query.plan.type"
      output: "db.query.plan.fingerprint"
    postgresql_rules:
      detection_jsonpath: "$.message"
      extractions:
        "db.query.plan.cost": '$.cost'
        "db.query.plan.rows": '$.rows'
        "db.query.plan.type": '$.type'
        "db.query.plan.startup_cost": '$.startup_cost'
    query_anonymization:
      enabled: true
      attributes_to_anonymize:
        - "db.statement"
        - "db.query.text"
        - "query"
      generate_fingerprint: true
      fingerprint_attribute: "db.query.fingerprint"
      
  # Adaptive sampler for cost control
  adaptivesampler:
    sampling_percentage: 20
    evaluation_interval: 180s  # 3 minutes
    max_traces_per_second: 100
    adjust_sampling_on_overload: true
    memory_threshold_mb: 128
    cost_control:
      max_cost_per_minute: 500
      enable_cost_alerts: true
      cost_calculation_method: "complexity_based"
      
  # Secure data transformation
  transform:
    error_mode: ignore
    metric_statements:
      - context: metric
        statements:
          - set(resource.attributes["service.instance.id"], Concat([Split("${env:HOSTNAME:-localhost}", ".")[0], "-secure"]))
          - set(resource.attributes["telemetry.sdk.name"], "opentelemetry")
          - set(resource.attributes["telemetry.sdk.language"], "go")
          - set(resource.attributes["telemetry.distro.name"], "database-intelligence")
          
    log_statements:
      - context: log
        statements:
          # Security: Remove potentially sensitive information
          - delete_key(attributes, "password") where attributes["password"] != nil
          - delete_key(attributes, "passwd") where attributes["passwd"] != nil
          - delete_key(attributes, "secret") where attributes["secret"] != nil
          - delete_key(attributes, "token") where attributes["token"] != nil
          - delete_key(attributes, "key") where attributes["key"] != nil
          - delete_key(attributes, "api_key") where attributes["api_key"] != nil
          
  # Efficient batching
  batch:
    timeout: 15s
    send_batch_size: 500
    send_batch_max_size: 1000
    
  # Filter for high-cardinality metrics
  filter/high_cardinality:
    error_mode: ignore
    metrics:
      metric:
        # Filter out potentially problematic high-cardinality metrics
        - 'name matches ".*\\.table\\..*" and HasAttrKeyOnDatapoint("table") == true'
        - 'name matches ".*\\.index\\..*" and HasAttrKeyOnDatapoint("index") == true'
        
exporters:
  # New Relic OTLP exporter
  otlp/newrelic:
    endpoint: "${env:NEW_RELIC_OTLP_ENDPOINT:-https://otlp.nr-data.net:4318}"
    headers:
      "api-key": "${env:NEW_RELIC_LICENSE_KEY:-dummy_key}"
      "user-agent": "database-intelligence-secure/1.0.0"
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 120s
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 1000
      
  # Prometheus exporter (restricted access)
  prometheus:
    endpoint: "127.0.0.1:8888"  # Localhost only
    const_labels:
      collector: "database-intelligence"
      environment: "${env:ENVIRONMENT:-development}"
      security_mode: "enabled"
    send_timestamps: true
    metric_expiration: 10m
    enable_open_metrics: false
    
  # Logging exporter
  logging:
    loglevel: "${env:LOG_LEVEL:-info}"
    sampling_initial: 2
    sampling_thereafter: 500

service:
  telemetry:
    logs:
      level: "${env:LOG_LEVEL:-info}"
      development: false
      sampling:
        enabled: true
        tick: 10s
        initial: 2
        thereafter: 100
    metrics:
      level: basic
      address: "127.0.0.1:8889"  # Internal metrics on localhost only
      
  extensions:
    - health_check
    
  pipelines:
    # Metrics pipeline with comprehensive processing
    metrics:
      receivers:
        - postgresql
        - mysql
        - sqlquery/postgresql_secure
      processors:
        - memory_limiter
        - circuitbreaker
        - resource
        - filter/high_cardinality
        - transform
        - batch
      exporters:
        - otlp/newrelic
        - prometheus
        - logging
        
    # Logs pipeline with security processing
    logs:
      receivers: []  # Add log receivers as needed
      processors:
        - memory_limiter
        - circuitbreaker
        - planattributeextractor
        - resource
        - transform
        - batch
      exporters:
        - otlp/newrelic
        - logging
        
    # Traces pipeline (minimal for security)
    traces:
      receivers: []  # Add trace receivers as needed
      processors:
        - memory_limiter
        - resource
        - batch
      exporters:
        - otlp/newrelic