# Database Intelligence Collector - Resilient Configuration (Fixed)
# This configuration maximizes reliability with graceful degradation

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
    
  zpages:
    endpoint: 0.0.0.0:55679

receivers:
  # PostgreSQL metrics using standard receiver
  postgresql:
    endpoint: ${env:POSTGRES_HOST:-localhost}:${env:POSTGRES_PORT:-5432}
    username: ${env:POSTGRES_USER:-postgres}
    password: ${env:POSTGRES_PASSWORD:-postgres}
    databases:
      - ${env:POSTGRES_DB:-postgres}
    collection_interval: 15s
    tls:
      insecure: true

  # MySQL metrics using standard receiver  
  mysql:
    endpoint: ${env:MYSQL_HOST:-localhost}:${env:MYSQL_PORT:-3306}
    username: ${env:MYSQL_USER:-root}
    password: ${env:MYSQL_PASSWORD:-mysql}
    database: ${env:MYSQL_DB:-mysql}
    collection_interval: 15s

  # SQL query receiver for PostgreSQL
  sqlquery/postgresql:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:-localhost} port=${env:POSTGRES_PORT:-5432} user=${env:POSTGRES_USER:-postgres} password=${env:POSTGRES_PASSWORD:-postgres} dbname=${env:POSTGRES_DB:-postgres} sslmode=disable"
    queries:
      # Basic query performance (pg_stat_statements required)
      - sql: |
          SELECT 
            COALESCE(queryid::text, 'unknown') as query_id,
            COALESCE(left(query, 100), 'unavailable') as query_text,
            COALESCE(calls, 0) as execution_count,
            COALESCE(total_exec_time, 0) as total_time_ms,
            COALESCE(mean_exec_time, 0) as avg_duration_ms,
            COALESCE(rows, 0) as rows_returned,
            current_database() as database_name,
            'postgresql' as db_system
          FROM pg_stat_statements
          WHERE calls > 0
          ORDER BY mean_exec_time DESC
          LIMIT 100
        logs:
          - body_column: query_text
            attributes:
              query_id: query_id
              avg_duration_ms: avg_duration_ms
              execution_count: execution_count
              total_time_ms: total_time_ms
              database_name: database_name
              db_system: db_system
              rows_returned: rows_returned
    collection_interval: 30s

  # SQL query receiver for MySQL
  sqlquery/mysql:
    driver: mysql
    datasource: "${env:MYSQL_USER:-root}:${env:MYSQL_PASSWORD:-mysql}@tcp(${env:MYSQL_HOST:-localhost}:${env:MYSQL_PORT:-3306})/${env:MYSQL_DB:-mysql}"
    queries:
      - sql: |
          SELECT
            COALESCE(DIGEST, 'unknown') as query_id,
            COALESCE(LEFT(DIGEST_TEXT, 100), 'unavailable') as query_text,
            COALESCE(COUNT_STAR, 0) as execution_count,
            COALESCE(ROUND((AVG_TIMER_WAIT * COUNT_STAR)/1000000, 2), 0) as total_time_ms,
            COALESCE(ROUND(AVG_TIMER_WAIT/1000000, 2), 0) as avg_duration_ms,
            COALESCE(SUM_ROWS_SENT, 0) as rows_returned,
            COALESCE(SCHEMA_NAME, 'unknown') as database_name,
            'mysql' as db_system
          FROM performance_schema.events_statements_summary_by_digest
          WHERE SCHEMA_NAME IS NOT NULL
            AND COUNT_STAR > 0
          ORDER BY AVG_TIMER_WAIT DESC
          LIMIT 100
        logs:
          - body_column: query_text
            attributes:
              query_id: query_id
              avg_duration_ms: avg_duration_ms
              execution_count: execution_count
              total_time_ms: total_time_ms
              database_name: database_name
              db_system: db_system
              rows_returned: rows_returned
    collection_interval: 30s

processors:
  # Memory limiter - first processor
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  # CRITICAL: Resource processor for dashboard compatibility
  resource:
    attributes:
      - key: collector.name
        value: otelcol
        action: upsert
      - key: service.name
        value: database-monitoring
        action: insert
      - key: deployment.environment
        value: ${env:ENVIRONMENT:-production}
        action: insert
      - key: collector.instance.id
        value: ${env:HOSTNAME}
        action: upsert

  # Enhanced PII protection
  transform/sanitize_pii:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          # Credit card numbers (various formats)
          - replace_all_patterns(attributes["query_text"], "\\b(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|3[47][0-9]{13}|3[0-9]{13}|6(?:011|5[0-9]{2})[0-9]{12})\\b", "****-****-****-XXXX")
          # Social Security Numbers
          - replace_all_patterns(attributes["query_text"], "\\b(?:\\d{3}[-.]?\\d{2}[-.]?\\d{4})\\b", "XXX-XX-XXXX")
          # Email addresses
          - replace_all_patterns(attributes["query_text"], "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b", "email@REDACTED.com")
          # Phone numbers
          - replace_all_patterns(attributes["query_text"], "\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b", "+1-XXX-XXX-XXXX")
          # Long strings that might contain PII
          - replace_all_patterns(attributes["query_text"], "'[^']{20,}'", "'LONG_STRING_REDACTED'")
          # Generic numeric sequences
          - replace_all_patterns(attributes["query_text"], "\\b\\d{6,}\\b", "XXXXXX")
          # Quoted identifiers
          - replace_all_patterns(attributes["query_text"], "\"[^\"]+\"", "\"[IDENTIFIER]\"")

  # Transform metrics to ensure proper types
  transform/metrics:
    error_mode: ignore
    metric_statements:
      - context: metric
        statements:
          - set(unit, "1") where unit == ""

  # Transform logs to ensure proper types
  transform/logs:
    error_mode: ignore
    log_statements:
      - context: log
        statements:
          - set(attributes["avg_duration_ms"], Double(attributes["avg_duration_ms"])) where attributes["avg_duration_ms"] != nil
          - set(attributes["execution_count"], Int(attributes["execution_count"])) where attributes["execution_count"] != nil
          - set(attributes["total_time_ms"], Double(attributes["total_time_ms"])) where attributes["total_time_ms"] != nil
          - set(attributes["rows_returned"], Int(attributes["rows_returned"])) where attributes["rows_returned"] != nil

  # Probabilistic sampling for high-volume scenarios
  probabilistic_sampler:
    hash_seed: 22
    sampling_percentage: 100  # Start with 100%, adjust based on volume

  # Batch processor
  batch:
    timeout: 10s
    send_batch_size: 1000
    send_batch_max_size: 2000

exporters:
  # Primary exporter to New Relic
  otlp/newrelic:
    endpoint: ${env:OTLP_ENDPOINT:-https://otlp.nr-data.net:4317}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
    compression: gzip
    timeout: 30s
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 120s
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 1000

  # Debug exporter for troubleshooting
  debug:
    verbosity: basic
    sampling_initial: 5
    sampling_thereafter: 100

  # File exporter for local backup
  file:
    path: /var/log/otel/metrics.json
    rotation:
      max_megabytes: 100
      max_days: 3
      max_backups: 2

service:
  extensions: [health_check, zpages]
  
  pipelines:
    # Database metrics pipeline
    metrics/databases:
      receivers: [postgresql, mysql]
      processors: [memory_limiter, resource, transform/metrics, batch]
      exporters: [otlp/newrelic, file]
      
    # Query logs pipeline
    logs/queries:
      receivers: [sqlquery/postgresql, sqlquery/mysql]
      processors: [memory_limiter, resource, transform/logs, transform/sanitize_pii, probabilistic_sampler, batch]
      exporters: [otlp/newrelic, file]
      
  telemetry:
    logs:
      level: info
      encoding: json
      output_paths: ["/var/log/otel/collector.log", "stdout"]
      error_output_paths: ["stderr"]
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
# Health check available at http://localhost:13133/
# zPages available at http://localhost:55679/debug/tracez
# Metrics available at http://localhost:8888/metrics