# Database Intelligence Collector - Resilient Configuration
# This configuration maximizes reliability with graceful degradation when processors are unavailable

extensions:
  health_check:
    endpoint: 0.0.0.0:13133

receivers:
  # PostgreSQL metrics using standard receiver
  postgresql:
    endpoint: ${env:POSTGRES_HOST:-localhost}:${env:POSTGRES_PORT:-5432}
    username: ${env:POSTGRES_USER:-postgres}
    password: ${env:POSTGRES_PASSWORD:-postgres}
    databases:
      - ${env:POSTGRES_DB:-postgres}
    collection_interval: 15s
    tls:
      insecure: true

  # SQL query receiver with fallback-compatible queries
  sqlquery:
    driver: postgres
    datasource: "host=${env:POSTGRES_HOST:-localhost} port=${env:POSTGRES_PORT:-5432} user=${env:POSTGRES_USER:-postgres} password=${env:POSTGRES_PASSWORD:-postgres} dbname=${env:POSTGRES_DB:-postgres} sslmode=disable"
    queries:
      # Basic query performance without complex dependencies
      - sql: |
          SELECT 
            COALESCE(queryid::text, 'unknown') as query_id,
            COALESCE(left(query, 100), 'unavailable') as query_text,
            COALESCE(calls, 0) as execution_count,
            COALESCE(total_exec_time, 0) as total_time_ms,
            COALESCE(mean_exec_time, 0) as avg_duration_ms,
            COALESCE(rows, 0) as rows_returned,
            current_database() as database_name,
            'postgresql' as db_system
          FROM pg_stat_statements
          WHERE calls > 0
          ORDER BY mean_exec_time DESC
          LIMIT 100
        metrics:
          - metric_name: db.query.calls
            value_column: execution_count
            attribute_columns: [query_id, database_name, db_system]
            value_type: int
          - metric_name: db.query.mean_time
            value_column: avg_duration_ms
            attribute_columns: [query_id, database_name, db_system]
            value_type: double
        collection_interval: 30s

processors:
  # Standard OTEL processors
  memory_limiter:
    check_interval: 1s
    limit_mib: 512

  batch:
    timeout: 10s
    send_batch_size: 1000

  resource:
    attributes:
      - key: collector.name
        value: otelcol
        action: upsert
      - key: service.name
        value: database-monitoring
        action: insert
      - key: deployment.environment
        value: ${env:ENVIRONMENT:-production}
        action: insert

  # Enhanced PII protection
  transform:
    error_mode: ignore
    metric_statements:
      - context: datapoint
        statements:
          # Credit card numbers (various formats)
          - replace_pattern(attributes["query_text"], "\\b(?:4[0-9]{12}(?:[0-9]{3})?|5[1-5][0-9]{14}|3[47][0-9]{13}|3[0-9]{13}|6(?:011|5[0-9]{2})[0-9]{12})\\b", "****-****-****-XXXX") where attributes["query_text"] != nil
          # Social Security Numbers
          - replace_pattern(attributes["query_text"], "\\b(?:\\d{3}[-.]?\\d{2}[-.]?\\d{4})\\b", "XXX-XX-XXXX") where attributes["query_text"] != nil
          # Email addresses
          - replace_pattern(attributes["query_text"], "\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b", "email@REDACTED.com") where attributes["query_text"] != nil
          # Phone numbers
          - replace_pattern(attributes["query_text"], "\\b(?:\\+?1[-.]?)?\\(?([0-9]{3})\\)?[-.]?([0-9]{3})[-.]?([0-9]{4})\\b", "+1-XXX-XXX-XXXX") where attributes["query_text"] != nil
          # Long strings that might contain PII
          - replace_pattern(attributes["query_text"], "'[^']{20,}'", "'LONG_STRING_REDACTED'") where attributes["query_text"] != nil
          # Generic numeric sequences
          - replace_pattern(attributes["query_text"], "\\b\\d{6,}\\b", "XXXXXX") where attributes["query_text"] != nil

  # Adaptive sampler with resilient rules (no plan dependency)
  adaptive_sampler:
    in_memory_only: true
    rules:
      - name: "slow_queries"
        priority: 100
        sample_rate: 1.0
        conditions:
          - attribute: "avg_duration_ms"
            operator: "gt"
            value: 1000.0
      - name: "medium_queries"
        priority: 50
        sample_rate: 0.5
        conditions:
          - attribute: "avg_duration_ms"
            operator: "gt"
            value: 100.0
      - name: "fast_queries"
        priority: 10
        sample_rate: 0.1
        conditions:
          - attribute: "avg_duration_ms"
            operator: "lte"
            value: 100.0
    default_sample_rate: 0.1
    max_records_per_second: 1000
    enable_debug_logging: true
    deduplication:
      enabled: false  # Disabled since we don't have plan hashes

  # Circuit breaker for database protection
  circuit_breaker:
    failure_threshold: 5
    success_threshold: 3
    open_state_timeout: 30s
    max_concurrent_requests: 100
    base_timeout: 5s
    max_timeout: 30s
    enable_adaptive_timeout: true
    health_check_interval: 10s
    memory_threshold_mb: 512
    cpu_threshold_percent: 80.0
    enable_debug_logging: true

exporters:
  # OTLP to New Relic
  otlp:
    endpoint: ${env:OTLP_ENDPOINT:-otlp.nr-data.net:4317}
    headers:
      api-key: ${env:NEW_RELIC_LICENSE_KEY}
    compression: gzip
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s

  # Debug for troubleshooting
  debug:
    verbosity: normal
    sampling_initial: 5
    sampling_thereafter: 100

  # Prometheus for monitoring
  prometheus:
    endpoint: 0.0.0.0:8889
    namespace: db_intelligence

service:
  extensions: [health_check]
  
  pipelines:
    # Resilient metrics pipeline with graceful degradation
    metrics/resilient:
      receivers: [postgresql, sqlquery]
      processors: 
        - memory_limiter
        - resource
        - transform
        - adaptive_sampler  # Works without plan attributes
        - circuit_breaker   # Independent protection
        - batch
      exporters: [otlp, prometheus, debug]

  telemetry:
    logs:
      level: ${env:LOG_LEVEL:-info}
    metrics:
      level: detailed
      address: 0.0.0.0:8888