# Complete OHI Migration Configuration
# This configuration provides full OHI feature parity with OpenTelemetry

# Extensions for debugging and health monitoring
extensions:
  healthcheck:
    endpoint: 0.0.0.0:13133
    
  zpages:
    endpoint: 0.0.0.0:55679
    
  pprof:
    endpoint: 0.0.0.0:1777

# Receivers for database metrics collection
receivers:
  # PostgreSQL standard metrics
  postgresql:
    endpoint: ${POSTGRES_HOST}:${POSTGRES_PORT}
    transport: tcp
    username: ${POSTGRES_USER}
    password: ${POSTGRES_PASSWORD}
    databases:
      - postgres
      - ${POSTGRES_DB}
    collection_interval: 15s  # Match OHI interval
    tls:
      insecure: true
      insecure_skip_verify: true
      
  # PostgreSQL query performance (replaces PostgresSlowQueries)
  sqlquery/postgresql_queries:
    driver: postgres
    datasource: "host=${POSTGRES_HOST} port=${POSTGRES_PORT} user=${POSTGRES_USER} password=${POSTGRES_PASSWORD} dbname=postgres sslmode=disable"
    collection_interval: 60s
    queries:
      # Include all queries from receiver-sqlquery-ohi.yaml
      - sql: |
          SELECT 
            queryid,
            LEFT(query, 100) as query_text,
            calls as execution_count,
            total_exec_time,
            mean_exec_time as avg_elapsed_time_ms,
            stddev_exec_time,
            rows,
            shared_blks_hit + shared_blks_read as total_blocks,
            shared_blks_hit,
            shared_blks_read,
            shared_blks_dirtied,
            shared_blks_written,
            local_blks_hit + local_blks_read as local_blocks,
            temp_blks_read + temp_blks_written as temp_blocks,
            blk_read_time + blk_write_time as io_time,
            userid,
            dbid,
            datname as database_name
          FROM pg_stat_statements 
          JOIN pg_database ON pg_database.oid = pg_stat_statements.dbid
          WHERE 
            calls > 20  -- OHI threshold
            AND mean_exec_time > 500  -- OHI threshold
            AND query NOT LIKE '%pg_stat%'
          ORDER BY mean_exec_time DESC
          LIMIT 100
        metrics:
          - metric_name: db.query.count
            value_column: execution_count
            value_type: int
            attributes:
              - queryid
              - database_name
              - query_text
            data_point_type: sum
            monotonic: true
            
  # MySQL standard metrics
  mysql:
    endpoint: ${MYSQL_HOST}:${MYSQL_PORT}
    username: ${MYSQL_USER}
    password: ${MYSQL_PASSWORD}
    database: ${MYSQL_DB}
    collection_interval: 15s  # Match OHI interval
    transport: tcp
    tls:
      insecure: true
      
  # MySQL query performance and InnoDB metrics
  sqlquery/mysql_queries:
    driver: mysql
    datasource: "${MYSQL_USER}:${MYSQL_PASSWORD}@tcp(${MYSQL_HOST}:${MYSQL_PORT})/"
    collection_interval: 60s
    queries:
      # Include queries from receiver-sqlquery-ohi.yaml for MySQL

# Processors for data transformation and enhancement
processors:
  # Memory protection
  memory_limiter:
    check_interval: 1s
    limit_percentage: 80
    spike_limit_percentage: 25
    
  # Add resource attributes for entity synthesis
  resource:
    attributes:
      - key: service.name
        value: database-monitoring
        action: insert
      - key: deployment.environment
        value: ${ENVIRONMENT}
        action: insert
      - key: telemetry.source
        value: otel
        action: insert
        
  # OHI metric transformations
  metricstransform/ohi_compatibility:
    transforms:
      # PostgreSQL transformations
      - include: postgresql.bgwriter.checkpoint.count
        action: update
        new_name: db.bgwriter.checkpointsScheduledPerSecond
        
      - include: postgresql.bgwriter.duration
        action: update
        new_name: db.bgwriter.checkpointWriteTimeInMillisecondsPerSecond
        operations:
          - action: experimental_scale_value
            scale: 1000
            
      - include: postgresql.bgwriter.buffers.writes
        action: update
        new_name: db.bgwriter.buffersWrittenByBackgroundWriterPerSecond
        
      - include: postgresql.commits
        action: update
        new_name: db.commitsPerSecond
        
      - include: postgresql.rollbacks
        action: update
        new_name: db.rollbacksPerSecond
        
      - include: postgresql.blocks_read
        action: update
        new_name: db.reads.blocksPerSecond
        
      - include: postgresql.blocks_written
        action: update
        new_name: db.writes.blocksPerSecond
        
      - include: postgresql.database.count
        action: update
        new_name: db.database.count
        
      - include: postgresql.database.size
        action: update
        new_name: db.database.sizeInBytes
        
      - include: postgresql.database.backends
        action: update
        new_name: db.connections.active
        
      # MySQL transformations
      - include: mysql.buffer_pool.data_pages
        action: update
        new_name: db.innodb.bufferPoolDataPages
        
      - include: mysql.buffer_pool.page_flushes
        action: update
        new_name: db.innodb.bufferPoolPagesFlushedPerSecond
        
      - include: mysql.query_cache.hits
        action: update
        new_name: db.queryCacheHitsPerSecond
        
      - include: mysql.handlers.write
        action: update
        new_name: db.handler.writePerSecond
        
  # Add OHI-compatible attributes
  attributes/ohi_compatibility:
    actions:
      - key: entity.name
        from_attribute: service.name
        action: insert
        
      - key: entity.type
        value: DatabaseNode
        action: insert
        
      - key: integration.name
        value: com.newrelic.otel
        action: insert
        
      - key: integration.version
        value: "2.0.0"
        action: insert
        
  # Query anonymization
  transform/query_anonymization:
    error_mode: ignore
    log_statements:
      - context: datapoint
        statements:
          - replace_pattern(attributes["query_text"], "\\b\\d+\\b", "?")
          - replace_pattern(attributes["query_text"], "'[^']*'", "'?'")
          - replace_pattern(attributes["query_text"], "\"[^\"]*\"", "\"?\"")
          - truncate_all(attributes["query_text"], 500)
          
  # Query correlation processor
  querycorrelator:
    retention_period: 24h
    cleanup_interval: 1h
    enable_table_correlation: true
    enable_database_correlation: true
    max_queries_tracked: 10000
    correlation_attributes:
      add_query_category: true
      add_table_stats: true
      add_load_contribution: true
      add_maintenance_indicators: true
      
  # Database protection
  circuitbreaker:
    failure_threshold: 10
    timeout_duration: 30s
    half_open_requests: 5
    recovery_duration: 60s
    
  # Intelligent sampling
  adaptivesampler:
    in_memory_only: true
    default_sample_rate: 0.1
    rules:
      - name: database_errors
        conditions:
          - attribute: db.system
            operator: exists
          - attribute: error
            operator: eq
            value: true
        sample_rate: 1.0
      - name: slow_queries
        conditions:
          - attribute: db.query.mean_duration
            operator: gt
            value: 1000
        sample_rate: 0.5
        
  # Data quality and compliance
  verification:
    pii_detection:
      enabled: true
      sensitivity: high
    quality_checks:
      enabled: true
      max_attribute_length: 1000
      max_attributes_per_span: 128
      
  # Cost control
  costcontrol:
    monthly_budget_usd: 5000
    price_per_gb: 0.35
    metric_cardinality_limit: 10000
    aggressive_mode: false
    
  # Integration error monitoring
  nrerrormonitor:
    max_attribute_length: 4095
    max_metric_name_length: 255
    cardinality_warning_threshold: 10000
    alert_threshold: 100
    
  # Batching for efficiency
  batch:
    timeout: 10s
    send_batch_size: 1000
    send_batch_max_size: 2000

# Exporters for data destinations
exporters:
  # New Relic OTLP exporter
  otlp/newrelic:
    endpoint: https://otlp.nr-data.net:4318
    headers:
      api-key: ${NEW_RELIC_LICENSE_KEY}
    sending_queue:
      enabled: true
      num_consumers: 10
      queue_size: 5000
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 300s
      
  # Local Prometheus for debugging
  prometheus:
    endpoint: 0.0.0.0:8889
    
  # Debug output
  debug:
    verbosity: detailed
    sampling_initial: 5
    sampling_thereafter: 200

# Service configuration with pipelines
service:
  # Enable extensions
  extensions: [healthcheck, zpages, pprof]
  
  # Telemetry for the collector itself
  telemetry:
    logs:
      level: info
      development: false
      encoding: json
    metrics:
      level: detailed
      address: 0.0.0.0:8888
      
  # Data pipelines
  pipelines:
    # PostgreSQL metrics pipeline
    metrics/postgresql:
      receivers: [postgresql, sqlquery/postgresql_queries]
      processors:
        - memory_limiter
        - resource
        - attributes/ohi_compatibility
        - metricstransform/ohi_compatibility
        - transform/query_anonymization
        - querycorrelator
        - nrerrormonitor
        - circuitbreaker
        - verification
        - adaptivesampler
        - costcontrol
        - batch
      exporters: [otlp/newrelic, prometheus]
      
    # MySQL metrics pipeline
    metrics/mysql:
      receivers: [mysql, sqlquery/mysql_queries]
      processors:
        - memory_limiter
        - resource
        - attributes/ohi_compatibility
        - metricstransform/ohi_compatibility
        - transform/query_anonymization
        - querycorrelator
        - nrerrormonitor
        - circuitbreaker
        - verification
        - adaptivesampler
        - costcontrol
        - batch
      exporters: [otlp/newrelic, prometheus]
      
    # Debug pipeline for troubleshooting
    metrics/debug:
      receivers: [postgresql, mysql]
      processors:
        - memory_limiter
        - batch
      exporters: [debug]